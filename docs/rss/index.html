<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Ashwin Mathur]]></title><description><![CDATA[AI Research, NLP, Open-Source]]></description><link>https://awinml.github.io/</link><image><url>https://awinml.github.io/favicon.png</url><title>Ashwin Mathur</title><link>https://awinml.github.io/</link></image><generator>Ghost 5.49</generator><lastBuildDate>Wed, 28 Jun 2023 18:51:02 GMT</lastBuildDate><atom:link href="https://awinml.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Machine Learning Model Deployment: From Jupyter Notebook to the Cloud]]></title><description><![CDATA[Building a machine learning model is only half the battle. Deploying the model into a production environment where it can be used to make predictions is equally important. In this article, we will explore the steps involved in deploying a machine learning model from a Jupyter Notebook to the cloud.]]></description><link>https://awinml.github.io/ml-model-deployment/</link><guid isPermaLink="false">648d7155fdfec6286dfa89d9</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Sat, 17 Jun 2023 08:46:10 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Machine Learning Model Deployment: From Jupyter Notebook to the Cloud"><p>Machine learning has become an essential tool for businesses and organizations to make data-driven decisions. However, building a machine learning model is only half the battle. Deploying the model into a production environment where it can be used to make predictions is equally important. In this article, we will explore the steps involved in deploying a machine learning model from a Jupyter Notebook to the cloud.</p><h2 id="developing-the-model">Developing the Model</h2><p>The first step in deploying a machine learning model is to develop the model itself. This is typically done in a Jupyter Notebook, where data scientists can test various modeling strategies and fine-tune the model&apos;s parameters. Once the model is developed and tested, it&apos;s time to move on to the next step.</p><h2 id="migrating-the-code">Migrating the Code</h2><p>The second step in deploying a machine learning model is to migrate the code from the Jupyter Notebook into executable modules. The idea here is to have a way to automate the whole model building and prediction process, which cannot be done on Jupyter. This involves creating preprocess, train, and inference Python scripts.</p><pre><code class="language-python"># Example of a preprocess script using scikit-learn pipelines
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

preprocess_pipeline = Pipeline([
    (&apos;imputer&apos;, SimpleImputer(strategy=&apos;median&apos;)),
    (&apos;scaler&apos;, StandardScaler())
])

preprocessed_data = preprocess_pipeline.fit_transform(data)
</code></pre><pre><code class="language-python"># Example of a train script using scikit-learn pipelines
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

train_pipeline = Pipeline([
    (&apos;preprocess&apos;, preprocess_pipeline),
    (&apos;classifier&apos;, RandomForestClassifier())
])

trained_model = train_pipeline.fit(X_train, y_train)
</code></pre><pre><code class="language-python"># Example of an inference script
def predict(model, data):
    # Model prediction code here
    preprocessed_data = preprocess_pipeline.transform(data)
    prediction = model.predict(preprocessed_data)
    return prediction
</code></pre><h2 id="saving-the-model-pipelines">Saving the Model Pipelines</h2><p>It is important to save the model pipelines after they have been trained. This is because the pipelines contain the preprocessing steps and the trained model, which are required for making predictions on new data. Saving the pipelines also allows for easy reusability of the model in other applications.</p><pre><code class="language-python"># Example of saving the trained model pipeline
import joblib

joblib.dump(train_pipeline, &apos;trained_model_pipeline.joblib&apos;)
</code></pre><pre><code class="language-python"># Example of loading the trained model pipeline
import joblib

trained_model_pipeline = joblib.load(&apos;trained_model_pipeline.joblib&apos;)
</code></pre><h2 id="building-an-api">Building an API</h2><p>The third step in deploying a machine learning model is to build an API that will take real-time inputs, usually in the form of JSON. This API could be user-facing, such as a mobile or web app, or an internal API that connects to another application to fetch the data.</p><pre><code class="language-python"># Example of a Flask API endpoint
@app.route(&apos;/predict&apos;, methods=[&apos;POST&apos;])
def predict():
    data = request.get_json()
    prediction = predict(trained_model, data)
    return jsonify(prediction.tolist())
</code></pre><h2 id="building-a-model-prediction-endpoint">Building a Model Prediction Endpoint</h2><p>The fourth step in deploying a machine learning model is to build a model prediction endpoint on the API that invokes the model&apos;s prediction API, something like the common <code>.predict()</code> or <code>.generate()</code> methods. The way the prediction works is when the input hits the <code>/predict</code> API (prediction endpoint), the inputs are passed to the model, and the API retrieves the model prediction (also usually in the form of JSON) like this: <code>{id: 0001, prediction_score: 55}</code>.</p><pre><code class="language-python"># Example of a model prediction endpoint
class ModelPredictionEndpoint:
    def __init__(self, model):
        self.model = model

    def predict(self, data):
        preprocessed_data = preprocess_pipeline.transform(data)
        prediction = self.model.predict(preprocessed_data)
        return prediction.tolist()
</code></pre><h2 id="wrapping-the-api">Wrapping the API</h2><p>The fifth step in deploying a machine learning model is to wrap the API using a container service like Docker so that the code is agnostic to the environment. This ensures that the API can be deployed on any platform without any issues.</p><pre><code class="language-dockerfile"># Example of a Dockerfile
FROM python:3.8-slim-buster
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre><h2 id="docker-and-kubernetes">Docker and Kubernetes</h2><p>Docker and Kubernetes are two popular tools used for deploying machine learning models to the cloud. Docker is a containerization platform that allows developers to package their applications and dependencies into a single container that can be run on any platform. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.</p><p>Using Docker and Kubernetes can simplify the deployment process and make it more scalable. Instead of deploying the application on a single server, Docker and Kubernetes allow for the deployment of multiple instances of the application, which can be scaled up or down based on demand.</p><pre><code class="language-dockerfile"># Example of a Dockerfile for a machine learning model
FROM python:3.8-slim-buster
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre><pre><code class="language-yaml"># Example of a Kubernetes deployment file for a machine learning model
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: my-registry/my-app:latest
        ports:
        - containerPort: 5000
</code></pre><h2 id="deploying-to-the-cloud">Deploying to the Cloud</h2><p>With the model API container, you can now push this same API to any cloud provider and their service offerings: AWS EC2, Sagemaker, Beanstalk, GCP Vertex AI, App Engine. Here&apos;s an example of deploying the Docker container on AWS Elastic Beanstalk:</p><ol><li>Create an Elastic Beanstalk environment with a Docker platform.</li><li>Upload the Docker image to a container registry like Docker Hub or Amazon ECR.</li><li>Configure the Elastic Beanstalk environment to use the Docker image.</li><li>Deploy the application to the Elastic Beanstalk environment.</li></ol><pre><code class="language-bash"># Example of deploying the Docker container on AWS Elastic Beanstalk
# Build the Docker image
docker build -t my-app .

# Tag the Docker image
docker tag my-app:latest my-registry/my-app:latest

# Push the Docker image to the container registry
docker push my-registry/my-app:latest

# Create an Elastic Beanstalk environment with a Docker platform
eb create my-environment --platform &quot;Docker 20.10.7&quot;

# Configure the Elastic Beanstalk environment to use the Docker image
eb setenv DOCKER_IMAGE=my-registry/my-app:latest

# Deploy the application to the Elastic Beanstalk environment
eb deploy
</code></pre><h2 id="conclusion">Conclusion</h2><p>In conclusion, deploying a machine learning model to the cloud involves several steps, including developing the model, migrating the code, building an API, building a model prediction endpoint, wrapping the API using a container service like Docker, and deploying to the cloud. Saving the model pipelines and using Docker and Kubernetes can simplify the deployment process and make it more scalable.</p>]]></content:encoded></item><item><title><![CDATA[Financial Dashboard for Market Intelligence]]></title><description><![CDATA[Built a end-to-end financial dashboard that collects and consolidates all of a business's critical observations in one place using the information obtained from the annual 10-K SEC Filings of 12 companies.]]></description><link>https://awinml.github.io/financial-dashboard-for-market-intelligence/</link><guid isPermaLink="false">64790593edcff63646236d5e</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:56:49 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/fin_dash_cover.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://awinml.github.io/content/images/2023/06/fin_dash_cover.png" alt="Financial Dashboard for Market Intelligence"><p>Built a end-to-end financial dashboard that collects and consolidates all of a business&apos;s critical observations in one place using the information obtained from the annual 10-K SEC Filings of 12 companies.</p>
</li>
<li>
<p>Collected text data from 10-K filings from SEC EDGAR using the <a href="https://sec-api.io/?ref=localhost">SEC ExtractorAPI</a>.</p>
</li>
<li>
<p>The filings of 12 companies spanning 5 sectors were collected for the duration of 2017 to 2021. Each filing had over 34,000 words.</p>
</li>
<li>
<p>The data was cleaned and transformed for Sentiment Analysis and Summarization. The data was manually labelled for both the tasks.</p>
</li>
<li>
<p>The <strong>RoBERTa, FinBERT and DistilBERT</strong> models were fine-tuned for sentiment analysis. The best results were obtained using the fine-tuned DistilBERT model. It achieved an <strong>Accuracy of 91.11% and an ROC-AUC Score of 0.972.</strong></p>
</li>
<li>
<p>The <strong>T5, DistilPEGASUS and DistilBART</strong> models were fine-tuned for summarization. The best results were obtained using the fine-tuned DistilBART model. It achieved an <strong>ROUGE-L Score of 67.7%.</strong></p>
</li>
<li>
<p>RAKE NLTK was used to identify important keywords from the generated summaries.</p>
</li>
<li>
<p>The Financial Dashboard was deployed as a web-app using Streamlit. It contains:</p>
<ul>
<li><strong>Insights and summaries</strong> for different sections from annual corporate filings.</li>
<li>Identification of <strong>important keywords</strong> mentioned in the report.</li>
<li><strong>Sentiment-based score</strong> that measures the company&apos;s performance over a certain time period.</li>
</ul>
</li>
</ul>
<p>The app can be viewed here: <a href="https://awinml-financial-market-intelligence-app-q6lj0g.streamlit.app/?ref=localhost">Financial Dashboard</a></p>
<!--kg-card-end: markdown--><h2 id="motivation"><strong>Motivation</strong></h2><p>In the current data driven world, it is essential to have access to the right information for impactful decision making. All publicly listed companies have to file annual reports to the government. These consolidated statements allow investors, financial analysts, business owners and other interested parties to get a complete overview of the company. Companies all over the world make key financial decisions based on annually released public filings.</p><p>These corporate filings are rife with complicated legal and financial jargon and make it practically impossible for a layman to understand. In most cases these documents have to be manually read and decoded by people with expert financial and legal understanding. The goal of this project is to develop a tool that automates this tedious procedure and makes it easier to acquire crucial financial information.</p><h2 id="data"><strong>Data</strong></h2><p>To extract the text from the SEC filing, the SEC&#x2019;s ExtractorAPI was used. The API can extract any text section from 10-Q, 10-K, and 8-K SEC filings, and returns the extracted content in cleaned and standardized text or HTML format.<br>The twelve companies for which the data has been collected as listed below organized by sector:</p><ol><li>Pharmaceutical:<br>Abbvie, Pfizer, Merck</li><li>Technology:<br>Alphabet, Meta, Microsoft</li><li>Retail:<br>Costco</li><li>Oil and Natural Gas:<br>Chevron</li><li>Food and Beverages:<br>Coca Cola, Pepsico</li></ol><p>Snapshot of the data:</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/data_snap.png" class="kg-image" alt="Financial Dashboard for Market Intelligence" loading="lazy" width="1330" height="602" srcset="https://awinml.github.io/content/images/size/w600/2023/06/data_snap.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/data_snap.png 1000w, https://awinml.github.io/content/images/2023/06/data_snap.png 1330w" sizes="(min-width: 720px) 720px"></figure><h2 id="sentiment-analysis"><strong>Sentiment Analysis</strong></h2><p>A local cross validation split was created by randomly sampling rows from the records of 12 companies across sectors like Technology, Finance, Retail and Pharma. <a href="https://github.com/awinml/financial-market-intelligence/blob/main/meta_10K.pdf?ref=localhost">A sample 10k report for Meta can be viewed here</a>.</p><p>The RoBERTa, FinBERT and DistilBERT models were fine-tuned for sentiment analysis. The best results were obtained using the fine-tuned <strong>DistilBERT</strong> model. It achieved an Accuracy of 91.11% and an ROC-AUC Score of 0.972.</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>F1</th>
<th>AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Roberta</td>
<td>0.662</td>
<td>0.656</td>
<td>0.628</td>
</tr>
<tr>
<td>FinBERT</td>
<td>0.746</td>
<td>0.682</td>
<td>0.721</td>
</tr>
<tr>
<td>DistilBERT</td>
<td>0.911</td>
<td>0.914</td>
<td>0.972</td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="summarization"><strong>Summarization</strong></h2><p>For the summarization task, the data of Pfizer, Costco and Meta was labeled and used. A local cross validation split was created by randomly sampling rows from the records of these companies.<br>Text summarization was carried out using these three transformers models:</p><p>The T5, DistilPEGASUS and DistilBART models were fine-tuned for summarization. The best results were obtained using the fine-tuned <strong>DistilBART</strong> model. It achieved an ROUGE-L Score of 67.7%.</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th>Model</th>
<th>ROUGUE-1</th>
<th>ROUGUE-2</th>
<th>ROUGUE-L</th>
<th>ROUGUE-LSUM</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5</td>
<td>32.22</td>
<td>28.5</td>
<td>31.5</td>
<td>31.5</td>
</tr>
<tr>
<td>DistilPEGASUS</td>
<td>48.32</td>
<td>34.48</td>
<td>43.51</td>
<td>31.50</td>
</tr>
<tr>
<td>DistilBART</td>
<td>72.28</td>
<td>61.15</td>
<td>67.70</td>
<td>71</td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="identifying-important-keywords"><strong>Identifying Important Keywords</strong></h2><p>RAKE NLTK was used to identify important keywords from the generated summaries.</p><h2 id="code"><strong>Code</strong>:</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/financial-market-intelligence?ref=localhost">Financial Dashboard for Market Intelligence - Github</a>.</p>]]></content:encoded></item><item><title><![CDATA[American Express - Default Prediction]]></title><description><![CDATA[Built a classification model to predict the probability that a customer does not pay back their credit card balance (defaults) based on their monthly customer statements using the data provided by American Express.]]></description><link>https://awinml.github.io/american-express-default-prediction/</link><guid isPermaLink="false">6479043fedcff63646236d20</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:53:59 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/amex_cover.jpg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://awinml.github.io/content/images/2023/06/amex_cover.jpg" alt="American Express - Default Prediction"><p>Built a <strong>classification model</strong> to predict the probability that a customer does not pay back their credit card balance (defaults) based on their monthly customer statements using the data provided by American Express.</p>
</li>
<li>
<p>The data was particularly challenging to deal with as it had <strong>5.5 million records</strong> and <strong>191 anonymized features</strong>. <strong>122 features</strong> had more than <strong>10% missing values</strong>. The target variable had <strong>severe class imbalance</strong>.</p>
</li>
<li>
<p>Engineered new features by taking different aggregations over time which helped <strong>increase model accuracy by 12%</strong>.</p>
</li>
<li>
<p><strong>Optimized XGBoost and LightGBM Classifiers using RandomSearchCV</strong> to reach the best model.</p>
</li>
<li>
<p>A Soft-Voting Ensemble of the best performing XGBoost and LightGBM Models was used to make final predictions which yielded an <strong>Accuracy of 94.48%, an F1-Score of 96.71% and an ROC-AUC Score of 96.40%.</strong></p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="data">Data</h2><p>Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics.</p><p>The dataset contains profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:</p><pre><code>D_* = Delinquency variables
S_* = Spend variables
P_* = Payment variables
B_* = Balance variables
R_* = Risk variables
</code></pre><p>with the following features being categorical:</p><p>&apos;B_30&apos;, &apos;B_38&apos;, &apos;D_114&apos;, &apos;D_116&apos;, &apos;D_117&apos;, &apos;D_120&apos;, &apos;D_126&apos;, &apos;D_63&apos;, &apos;D_64&apos;, &apos;D_66&apos;, &apos;D_68&apos;</p><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/amex-default-prediction?ref=localhost">here</a>.</p><h2 id="analysis">Analysis</h2><p>The complete analysis can be viewed <a href="https://nbviewer.org/github/awinml/amex-default-classification/blob/main/amex-eda.ipynb?ref=localhost">here</a>.</p><h3 id="target-distribution">Target Distribution</h3><ul><li>In the data present we observe that <strong>25.9% of records have defaulted</strong> on their credit card payments whereas <strong>74.1% have paid</strong> their bills on time.</li><li>This distribution shows us that there is <strong>severe class imbalance</strong> present.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/0_target_dist.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="452" height="343"></figure><h3 id="distribution-of-number-of-defaults-per-day-for-the-first-month">Distribution of Number of Defaults per day for the first Month:</h3><p>The <strong>proportion of customers that defualt is consistent across each day</strong> in the data, with a slight <strong>weekly seasonal trend</strong> influenced by the day when the customers receive their statements.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/0.1_statements_default.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="1303" height="432" srcset="https://awinml.github.io/content/images/size/w600/2023/06/0.1_statements_default.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/0.1_statements_default.png 1000w, https://awinml.github.io/content/images/2023/06/0.1_statements_default.png 1303w" sizes="(min-width: 720px) 720px"></figure><h3 id="frequency-of-customer-statements-for-the-first-month">Frequency of Customer Statements for the first month:</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/1_statements_over_time.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="725" height="458" srcset="https://awinml.github.io/content/images/size/w600/2023/06/1_statements_over_time.png 600w, https://awinml.github.io/content/images/2023/06/1_statements_over_time.png 725w" sizes="(min-width: 720px) 720px"></figure><ul><li>There is <strong>weekly seasonal pattern</strong> observed in the number of statements received per day.</li><li>As seen above this trend does not seem to be significantly affecting the proportion of default.</li></ul><h3 id="distribution-of-values-of-payment-variables">Distribution of values of Payment Variables:</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/3_distribution_payment.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="1144" height="286" srcset="https://awinml.github.io/content/images/size/w600/2023/06/3_distribution_payment.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/3_distribution_payment.png 1000w, https://awinml.github.io/content/images/2023/06/3_distribution_payment.png 1144w" sizes="(min-width: 720px) 720px"></figure><ul><li>We notice that <strong>Payment 2 is heavily negatively skewed (left skewed).</strong></li><li>Even though Payment 4 have continuous values between 0 and 1, most of the density is <strong>clustered around 0 and 1</strong>.</li><li>This tells us that there may be some <strong>Gaussian Noise present</strong>. The noise can be removed and into a binary variable.</li></ul><h3 id="correlation-of-features-with-target-variable">Correlation of Features with Target Variable:</h3><ul><li><strong>Payment 2</strong> is <strong>negatively correlated</strong> with the target with a correlation of <strong>-0.67.</strong></li><li><strong>Delinquency 48</strong> is <strong>positively correlated</strong> with the target with a correlation of <strong>0.61.</strong></li></ul><h3 id="correlation-of-payment-variables-with-target">Correlation of Payment Variables with Target</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/4_payment_corr.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="417" height="383"></figure><ul><li>We observe that Payment 2 and Target are <strong>highly negatively correlated</strong>.</li><li>This could be probably be due to the fact that people paying their bill have a less chance of default.</li></ul><h2 id="experiments">Experiments:</h2><!--kg-card-begin: markdown--><ul>
<li>
<p>There is a substantial number of missing values in the data. These cannot be imputed since the features are anonymized and there is no clear rationale behind imputation. This constraint forces us to choose models that can handle missing values.</p>
</li>
<li>
<p>There is a <strong>high cardinality of features ie. 191 features</strong> in the data. The presence of missing values restricts the usage of traditional dimensionality reduction techniques like PCA as well as feature selection methods like RFE.</p>
</li>
<li>
<p>Instead we have engineer new features using aggregations over the time dimension. As the aggregations ignore missing values, the engineered features are dense and can be used for modelling.</p>
</li>
<li>
<p>Some of the prominent models that are used for classification and accept inputs with missing values are XGBoost, LightGBM, and CatBoost. They all internally impute the data depending on whatever imputation technique delivers the greatest performance benefit.</p>
</li>
<li>
<p>A baseline was created using a XGBoost model with default hyperparameters which yielded an <strong>Accuracy of 78.84%, an F1-Score of 54.64% and an ROC-AUC Score of 65.72%.</strong></p>
</li>
<li>
<p>The LightGBM model with default hyperparameters was tried after that and improved <strong>Accuracy by 1%, F1-Score by 12% and ROC-AUC Score by 6%.</strong></p>
</li>
<li>
<p>A <strong>Randomized Grid Search</strong> with 5 Cross Validation folds was carried out to fine tune the XGBoost and LightGBM models.</p>
</li>
<li>
<p>Hyperparameters of the XGBoost model such as <code>n_estimators</code>, <code>max_depth</code> and <code>learning_rate</code> were tuned to improve <strong>Accuracy by 9%, F1-Score by 18% and ROC-AUC Score by 3%.</strong></p>
</li>
<li>
<p>Hyperparameters of the LightGBM model such as <code>n_estimators</code>, <code>feature_fraction</code> and <code>learning_rate</code> were tuned to improve <strong>Accuracy by 0.1%, F1-Score by 6% and ROC-AUC Score by 10%.</strong></p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="results">Results:</h2><p>A <strong>Soft Voting Classifier</strong> was used to create a ensemble of both the models and was used for generating the final predictions. It achieved an <strong>Accuracy of 94.48%, an F1-Score of 96.71% and an ROC-AUC Score of 96.40%.</strong></p><p>The results from all the models have been summarized below:</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th style="text-align: center"><strong>Model</strong></th>
<th style="text-align: center"><strong>Accuracy</strong></th>
<th style="text-align: center"><strong>F1-Score</strong></th>
<th style="text-align: center"><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><strong>XGBoost</strong> <em>(default)</em></td>
<td style="text-align: center">78.84</td>
<td style="text-align: center">54.64</td>
<td style="text-align: center">65.72</td>
</tr>
<tr>
<td style="text-align: center"><strong>LightGBM</strong> <em>(default)</em></td>
<td style="text-align: center">79.84</td>
<td style="text-align: center">62.92</td>
<td style="text-align: center">71.86</td>
</tr>
<tr>
<td style="text-align: center"><strong>XGBoost</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">88.61</td>
<td style="text-align: center">80.74</td>
<td style="text-align: center">74.96</td>
</tr>
<tr>
<td style="text-align: center"><strong>LightGBM</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">88.72</td>
<td style="text-align: center">86.42</td>
<td style="text-align: center">84.22</td>
</tr>
<tr>
<td style="text-align: center"><strong>Voting Classifier</strong> <em>(XGB + LGBM)</em></td>
<td style="text-align: center"><strong>94.48</strong></td>
<td style="text-align: center"><strong>96.72</strong></td>
<td style="text-align: center"><strong>96.40</strong></td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/amex-default-classification?ref=localhost">American Express - Default Prediction Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Generate features:</li></ol><pre><code class="language-bash">  python amex-feature-engg.py
</code></pre><ol><li>Fine-tune models:</li></ol><pre><code class="language-bash">  python amex-fine-tuning.py
</code></pre><ol><li>Generate predictions:</li></ol><pre><code class="language-bash">  python amex-final-prediction.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[H&M Personalized Product Recommendations]]></title><description><![CDATA[Built a product recommendation system to recommend products based on previous transactions, as well as from customer and product meta data using the data provided by H&M.]]></description><link>https://awinml.github.io/h-m-personalized-product-recommendations/</link><guid isPermaLink="false">6478fc81edcff63646236cd6</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:24:21 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/hm_recsys_cover-1.jpg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://awinml.github.io/content/images/2023/06/hm_recsys_cover-1.jpg" alt="H&amp;M Personalized Product Recommendations"><p>Built a <strong>product recommendation system</strong> to recommend products based on previous transactions, as well as from customer and product meta data using the data provided by H&amp;M.</p>
</li>
<li>
<p>The data contains <strong>1,05,542 unique products</strong> with information on <strong>24 characteristics</strong> for each product.</p>
</li>
<li>
<p>The data contains information on <strong>13,71,980 consumers</strong> and <strong>3,17,88,324 client transactions</strong> from 2018 to 2020.</p>
</li>
<li>
<p>A custom lightweight candidate retrieval method was created using a combination of retrieval of candidates that were purchased together in the last week as well as<br>
most popular candidates based on age group.</p>
</li>
<li>
<p>The candidates were ranked using a LightGBM model based on features created using the frequency of product purchase as well as the percentage of customers that purchased that product.</p>
</li>
<li>
<p>A fine tuned recommendation system using a custom candidate retrieval method and LightGBM Ranking model was used to make final predictions which yielded an <strong>MAP@12 score of 0.345 and an overall AUC of 0.76.</strong></p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="data">Data</h2><p>The purchase history of customers across time, along with supporting metadata has been provided. The goal is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends.</p><p>Files provided:</p><ul><li>articles.csv - detailed metadata for each article_id available for purchase</li><li>customers.csv - metadata for each customer_id in dataset</li><li>transactions.csv - data consisting of the purchases each customer for each date, as well as additional information. Duplicate rows correspond to multiple purchases of the same item.</li></ul><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations?ref=localhost">here</a>.</p><h2 id="analysis">Analysis</h2><p>The complete analysis can be viewed <a href="https://nbviewer.org/github/awinml/hm-recsys/blob/main/hm-eda.ipynb?ref=localhost">here</a>.</p><h3 id="distribution-of-number-of-transactions-per-day">Distribution of number of Transactions per day:</h3><ul><li>October 2019 recorded the highest number of transactions in duration of 2018 to 2020.</li><li>There is a quarterly seasonal spike of tranactions.</li><li>There tends to be a large number of transactions in the month of December every year.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/9_daily_transactions.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1189" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/9_daily_transactions.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/9_daily_transactions.png 1000w, https://awinml.github.io/content/images/2023/06/9_daily_transactions.png 1189w" sizes="(min-width: 720px) 720px"></figure><h3 id="distribution-of-number-of-transactions-per-day-grouped-by-sales-channel">Distribution of number of Transactions per day grouped by Sales Channel:</h3><ul><li>Sales Channel 1 has faily consistent number of transactions per day with rarely any large spikes.</li><li>Sales Channel 2 consistently outperforms Sales Channel 1 throughout 2018 to 2020.</li><li>The quarterly seasonal spike of tranactions is caused by transactions through Sales Channel 2.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/10_daily_transactions_sales_channel.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="966" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/10_daily_transactions_sales_channel.png 600w, https://awinml.github.io/content/images/2023/06/10_daily_transactions_sales_channel.png 966w" sizes="(min-width: 720px) 720px"></figure><h3 id="distribution-of-number-of-unique-articles-sold-per-day-grouped-by-sales-channel">Distribution of number of unique Articles sold per day grouped by Sales Channel:</h3><ul><li>Sales Channel 1 has faily consistent number of unique Articles sold per day with rarely any spikes.</li><li>Sales Channel 2 consistently sells more unique products per day than Sales Channel 1 throughout 2018 to 2020.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/11_daily_transactions_sales_channel_unique_articles.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="959" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/11_daily_transactions_sales_channel_unique_articles.png 600w, https://awinml.github.io/content/images/2023/06/11_daily_transactions_sales_channel_unique_articles.png 959w" sizes="(min-width: 720px) 720px"></figure><p>After seeing the distrubtion of transactions and unique articles sold per day, we get the intuition that Sales Channel 1 customers are more consistent and conservative buyers.</p><p>On the other hand, customers that use Sales Channel 2 are ready to try out new products and also purchase products only in specific months during the year.</p><h3 id="distribution-of-customers-across-age-group">Distribution of Customers across Age Group:</h3><ul><li>The highest number of customers are aged 21 years.</li><li>A large proportion of the customer demographic are young adults aged 19 to 26.</li><li>There is also a significant customer base that is aged 46 to 56 years.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/7_customer_age_distribution.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1182" height="399" srcset="https://awinml.github.io/content/images/size/w600/2023/06/7_customer_age_distribution.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/7_customer_age_distribution.png 1000w, https://awinml.github.io/content/images/2023/06/7_customer_age_distribution.png 1182w" sizes="(min-width: 720px) 720px"></figure><h3 id="distribution-of-customers-who-have-subscribed-for-fashion-news-alerts">Distribution of Customers who have subscribed for Fashion News Alerts:</h3><ul><li>67% of all the customers have not subscribed for Fashion News Alerts.</li><li>32% of the customers have subscribed for regular updates.</li><li>1% of the customers have subscribed for monthly updates</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/8_customers_fashion_news.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="410" height="387"></figure><h3 id="product-groups-with-the-highest-number-of-product-types">Product Groups with the highest number of Product Types:</h3><p>The product group &apos;Accessories&apos; has the highest number of product types followed by &apos;Shoes&apos; and &apos;Upper Body Garments&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/1_product_types.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="600" height="387" srcset="https://awinml.github.io/content/images/2023/06/1_product_types.png 600w"></figure><h3 id="product-types-with-the-highest-number-of-unique-articles">Product Types with the highest number of unique articles:</h3><p>The product type &apos;Trousers&apos; has the highest number of unique articles closely followed by &apos;Dress&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/2_popular_articles.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1277" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/2_popular_articles.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/2_popular_articles.png 1000w, https://awinml.github.io/content/images/2023/06/2_popular_articles.png 1277w" sizes="(min-width: 720px) 720px"></figure><h3 id="product-departments-with-the-highest-number-of-unique-articles">Product Departments with the highest number of unique articles:</h3><p>The product type &apos;Jersey&apos; has the highest number of unique articles closely followed by &apos;Knitwear&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/3_highest_articles_dept.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1045" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/3_highest_articles_dept.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/3_highest_articles_dept.png 1000w, https://awinml.github.io/content/images/2023/06/3_highest_articles_dept.png 1045w" sizes="(min-width: 720px) 720px"></figure><h3 id="product-graphical-appearance-names-with-the-highest-number-of-unique-articles">Product Graphical Appearance Names with the highest number of unique articles:</h3><p>The highest number of articles are of &apos;Solid&apos; appearance followed by &apos;All over pattern&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/4_highest_articles_name.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1024" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/4_highest_articles_name.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/4_highest_articles_name.png 1000w, https://awinml.github.io/content/images/2023/06/4_highest_articles_name.png 1024w" sizes="(min-width: 720px) 720px"></figure><h3 id="product-index-with-the-highest-number-of-unique-articles">Product Index with the highest number of unique articles:</h3><p>The index named &apos;Ladiesear&apos; has the highest number of unique articles closely followed by &apos;Divided&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/5_highest_articles_index_name-1.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="643" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/5_highest_articles_index_name-1.png 600w, https://awinml.github.io/content/images/2023/06/5_highest_articles_index_name-1.png 643w"></figure><h3 id="product-colour-group-names-with-the-highest-number-of-unique-articles">Product Colour Group Names with the highest number of unique articles:</h3><p>The highest number of articles are of &apos;Black&apos; colour group followed by &apos;Dark Blue&apos; and &apos;White&apos;.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/6_highest_articles_color_group.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="784" height="931" srcset="https://awinml.github.io/content/images/size/w600/2023/06/6_highest_articles_color_group.png 600w, https://awinml.github.io/content/images/2023/06/6_highest_articles_color_group.png 784w" sizes="(min-width: 720px) 720px"></figure><h2 id="experiments">Experiments:</h2><p>Intuition behind the custom retrieval strategy can be found <a href="https://nbviewer.org/github/awinml/hm-recsys/blob/main/hm-similar-items-explore.ipynb?ref=localhost">here.</a></p><h3 id="candidate-retrieval">Candidate Retrieval</h3><p>A lightweight candidate retrieval method was created that was a combination of the following retrieval strategies.</p><!--kg-card-begin: markdown--><ul>
<li>
<p>Recommend Items Purchased Together in the last week:</p>
<ul>
<li>Get 5 pairs of each article that were sold in the past week.</li>
<li>Ignore any article that wasn&apos;t sold within the past week.</li>
<li>Ignored any pair purchased by less than 2 customers.</li>
</ul>
</li>
<li>
<p>Recommend Items Purchased Together in the last few weeks:</p>
<ul>
<li>The number of previous weeks was tuned.</li>
</ul>
</li>
<li>
<p>Recommend most popular items based on age group</p>
</li>
</ul>
<!--kg-card-end: markdown--><p>This is a manual, time-aware collaborative filtering method so it includes trend information as well.</p><h3 id="candidate-ranking">Candidate Ranking:</h3><p>Feature creation for ranking based on:</p><ul><li>Percentage of customers the pair was based on.</li><li>How recently the article was purchased.</li><li>Number of times the pair of products was purchased.</li></ul><p>A LightGBM ranking model was used to rank the candidates.</p><ul><li>Hyperparameters of the LightGBM model such as <code>n_estimators</code>, and <code>num_leaves</code> were tuned.</li></ul><h2 id="results">Results:</h2><p>The fine tuned recommendation system using the custom candidate retrival method and LightGBM Ranking model was used to make final predictions which yielded an <strong>MAP@12 score of 0.345 and a overall AUC of 0.76.</strong></p><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/hm-recsys?ref=localhost">H&amp;M Personalized Product Recommendations Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Generate local cv:</li></ol><pre><code class="language-bash">  python hm-cv.py
</code></pre><ol><li>Fine-tune models and generate predictions:</li></ol><pre><code class="language-bash">  python hm-custom-retrieval-pred.py.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[Jigsaw - Multilingual Toxic Comment Classification]]></title><description><![CDATA[Built a multilingual text classification model to predict the probability that a comment is toxic using the data provided by Google Jigsaw.]]></description><link>https://awinml.github.io/jigsaw-multilingual-toxic-comment-classification/</link><guid isPermaLink="false">6478fb9eedcff63646236cbe</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:14:29 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/jigsaw_clf_cover.jpg" medium="image"/><content:encoded><![CDATA[<ul><li>Built a <strong>multilingual text classification model</strong> to predict the probability that a comment is toxic using the data provided by Google Jigsaw.</li><li>The data had <strong>4,35,775</strong> text comments in <strong>7 different languages</strong>. </li><li>A RNN model was used as a baseline. The <strong>BERT-Multilingual-base and XLMRoBERTa</strong> models were fine-tuned to get the best results. </li><li>The best results were obtained using the fine-tuned XLMRoberta model. It achieved an <strong>Accuracy of 96.24% and an ROC-AUC Score of 93.92%.</strong></li></ul><h2 id="data">Data</h2><img src="https://awinml.github.io/content/images/2023/06/jigsaw_clf_cover.jpg" alt="Jigsaw - Multilingual Toxic Comment Classification"><p>It only takes one toxic comment to sour an online discussion. Toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet. </p><p>The goal is to find the probability that a comment is toxic.</p><h4 id="columns-in-the-dataset">Columns in the dataset:</h4><pre><code>id - identifier within each file.
comment_text - the text of the comment to be classified.
lang - the language of the comment.
toxic - whether or not the comment is classified as toxic.
</code></pre><p>The comments are composed of multiple non-English languages and come either from Civil Comments or Wikipedia talk page edits.</p><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification?ref=localhost">here</a>.</p><h2 id="experiments">Experiments:</h2><h4 id="rnn"><strong>RNN:</strong></h4><ul><li>A baseline was created using the RNN model. An <strong>embedding layer of size 64</strong> was used. Training the model with an <strong>Adam optimizer with learning rate of 0.001</strong> for <strong>5 epochs</strong> yielded an <strong>Accuracy of 83.68% and an ROC-AUC Score of 55.72%.</strong></li></ul><h4 id="bert-multilingual-base"><strong>BERT-Multilingual-base:</strong></h4><ul><li>The BERT-Multilingual-base was fine tuned on the data. A <strong>hidden layer of 1024 neurons</strong> was added to the model. Training the model with an <strong>Adam optimizer with learning rate of 0.001, weight decay of 1e-6</strong> for <strong>10 epochs</strong> yielded an <strong>Accuracy of 93.92% and an ROC-AUC Score of 89.55%.</strong></li></ul><h4 id="xlm-roberta"><strong>XLM RoBERTa:</strong></h4><ul><li>The XLMRoberta model was fine tuned on the data. Training the model with an <strong>AdamW optimizer with learning rate of 1e-5, weight decay of 1e-5</strong> for <strong>7 epochs</strong> yielded an <strong>Accuracy of 96.24% and an ROC-AUC Score of 93.92%.</strong></li></ul><p>For all the models that were fine-tuned:</p><ul><li>Batch size of 64 was used for training.</li><li>Binary Cross-Entropy was used as the loss function.</li></ul><h2 id="results">Results:</h2><p>The best results were obtained using a fine-tuned XLMRoberta model. It was used for generating the final predictions. It achieved an <strong>Accuracy of 96.24% and an ROC-AUC Score of 93.92%.</strong></p><p>The results from all the models have been summarized below:</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th style="text-align: center"><strong>Model</strong></th>
<th style="text-align: center"><strong>Accuracy</strong></th>
<th style="text-align: center"><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><strong>RNN</strong></td>
<td style="text-align: center">83.68</td>
<td style="text-align: center">55.72</td>
</tr>
<tr>
<td style="text-align: center"><strong>BERT-Multilingual-base</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">93.92</td>
<td style="text-align: center">89.55</td>
</tr>
<tr>
<td style="text-align: center"><strong>XLM RoBERTa</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center"><strong>96.24</strong></td>
<td style="text-align: center"><strong>93.92</strong></td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/jigsaw-toxic-comment-clf?ref=localhost">Jigsaw - Multilingual Toxic Comment Classification Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Baseline model:</li></ol><pre><code class="language-bash">  python toxic-baseline-rnn.py
</code></pre><ol><li>Fine-tune models:</li></ol><pre><code class="language-bash">  python toxic-bertm-base.py
  python toxic-xlm-roberta.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[Sports Image Classification]]></title><description><![CDATA[Built a image classification model to predict the sport that is being represented in the image.]]></description><link>https://awinml.github.io/sports-image-classification/</link><guid isPermaLink="false">6478fa5eedcff63646236ca9</guid><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:10:03 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/sports_img_clf_cover.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://awinml.github.io/content/images/2023/06/sports_img_clf_cover.png" alt="Sports Image Classification"><p>Have you ever wondered how machines can recognize sports just by looking at images? Well, it&apos;s all thanks to the power of Convolutional Neural Networks (CNNs). In this article, we will explore how CNNs can be used to classify sports images and compare the performance of different CNN architectures.</p>
<h2 id="dataset">Dataset</h2>
<p>The dataset used for this project is a collection of images representing 100 different types of sports and activities. The sports range from traditional sports like &quot;archery&quot;, &quot;arm wrestling&quot;, &quot;bowling&quot;, &quot;football&quot;, &quot;water polo&quot;, &quot;weightlifting&quot; to non-traditional ones like &quot;wingsuit flying&quot; and &quot;nascar racing&quot;. The goal is to predict the correct sport based on the image.</p>
<p>The dataset consists of 13572 train, 500 test, and 500 validation images. Each image is of size 224 x 224 pixels and has been segregated into train, test, and valid directories. The dataset can be downloaded from <a href="https://www.kaggle.com/datasets/gpiosenka/sports-classification?ref=localhost">Kaggle</a>.</p>
<h2 id="experiments">Experiments</h2>
<p>To classify the sports images, we compared the performance of five different CNN architectures: a custom CNN, InceptionV3, ResNet50V2, MobileNetV2, and EfficientNetB3. For each model, we fine-tuned the pre-trained ImageNet weights and added two hidden layers of 256 and 128 neurons respectively with leaky-relu activations. Dropout layers with p=0.1 were added to prevent overfitting. The number of epochs was 50 with early stopping with a patience parameter of 2 epochs. A batch size of 32 was used for training, and Sparse Categorical Cross-Entropy was used as the loss function.</p>
<h3 id="custom-cnn">Custom CNN</h3>
<p>We started with a baseline custom CNN model with 3 convolution layers and 3 dense layers. A kernel of size 3 x 3 was used for all the convolution layers. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 47 epochs yielded an Accuracy of 56.44%, F1-Score of 48.48%, and an ROC-AUC Score of 49.46%.</strong></p>
<h3 id="inceptionv3">InceptionV3</h3>
<p>The InceptionV3 model was initialized with pre-trained ImageNet weights. Only the Dense layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 22 epochs yielded an Accuracy of 68.92%, F1-Score of 64.92%, and an ROC-AUC Score of 66.64%.</strong></p>
<h3 id="resnet50v2">ResNet50V2</h3>
<p>The ResNet50V2 model was initialized with pre-trained ImageNet weights. Only the Dense layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 16 epochs yielded an Accuracy of 72.88%, F1-Score of 70.67%, and an ROC-AUC Score of 69.72%.</strong></p>
<h3 id="mobilenetv2">MobileNetV2</h3>
<p>The MobileNetV2 model was initialized with pre-trained ImageNet weights, and all the layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 8 epochs yielded an Accuracy of 86.68%, F1-Score of 86.79%, and an ROC-AUC Score of 88.36%.</strong></p>
<h3 id="efficientnetb3">EfficientNetB3</h3>
<p>The EfficientNetB3 model was initialized with pre-trained ImageNet weights, and all the layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 18 epochs yielded an Accuracy of 92.72%, F1-Score of 91.76%, and an ROC-AUC Score of 96.92%.</strong></p>
<h2 id="results">Results</h2>
<p><strong>The best results were obtained using a fine-tuned EfficientNetB3 model, which achieved an Accuracy of 92.72%, F1-Score of 91.76%, and an ROC-AUC Score of 96.92%.</strong> The results from all the models have been summarized in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>Model</strong></th>
<th style="text-align:center"><strong>Accuracy</strong></th>
<th style="text-align:center"><strong>F1-Score</strong></th>
<th><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Custom CNN</strong></td>
<td style="text-align:center">56.44</td>
<td style="text-align:center">48.48</td>
<td>49.46</td>
</tr>
<tr>
<td style="text-align:center"><strong>InceptionV3</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">68.92</td>
<td style="text-align:center">64.92</td>
<td>66.64</td>
</tr>
<tr>
<td style="text-align:center"><strong>ResNet50V2</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">72.88</td>
<td style="text-align:center">70.67</td>
<td>69.72</td>
</tr>
<tr>
<td style="text-align:center"><strong>MobileNetV2</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">86.68</td>
<td style="text-align:center">86.79</td>
<td>88.36</td>
</tr>
<tr>
<td style="text-align:center"><strong>EfficientNetB3</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">92.72</td>
<td style="text-align:center">91.76</td>
<td>96.92</td>
</tr>
</tbody>
</table>
<h2 id="deploying-the-model">Deploying the Model</h2>
<p>A web app was made using Streamlit to make predictions for new images using the best model. The live app can be viewed <a href="https://huggingface.co/spaces/awinml/sports_classification?ref=localhost">here</a>.</p>
<h2 id="run-locally">Run Locally</h2>
<p>All the code for this project can be found in this <a href="https://github.com/awinml/sports-image-classification?ref=localhost">Github repository</a>. To run the app locally, you can follow the instructions provided in the repository.</p>
<ol>
<li>Install required libraries:<pre><code class="language-bash">  pip install -r streamlit/requirements.txt
</code></pre>
</li>
<li>Fine-tune models:<pre><code class="language-bash">  python sports-clf-custom-cnn.py
  python sports-clf-inception.py
  python sports-clf-resnet.py
  python sports-clf-mobilenet.py
  python sports-clf-efficientnet.py
</code></pre>
</li>
<li>Generate predictions:<pre><code class="language-bash">  python sports-clf-final-predictions.py
</code></pre>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, we have seen how CNNs can be used to classify sports images with high accuracy. We compared the performance of five different CNN architectures and found that the EfficientNetB3 model achieved the best results. This model can be used to classify sports images in real-time, which can be useful in various applications such as sports analytics, sports broadcasting, and sports betting.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[An Introduction to GitHub and Git]]></title><description><![CDATA[Learn about the fundamentals of Git and GitHub in this introductory article, including what they are, how they work, and how they can benefit your development workflow.]]></description><link>https://awinml.github.io/an-introduction-to-github-and-git/</link><guid isPermaLink="false">6478f396edcff63646236c7d</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 19:45:48 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/cover.jpg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://awinml.github.io/content/images/2023/06/cover.jpg" alt="An Introduction to GitHub and Git"><p>Git and GitHub are two of the most popular tools used by developers worldwide. Git is a distributed version control system that allows developers to track changes in their codebase, while GitHub is a web-based platform that provides a centralized location for hosting Git repositories. In this article, we will explore the fundamentals of using Git and GitHub, including how to set up a repository, commit changes, and collaborate with other developers. We will also explore some of the most important Git commands that every developer should know.</p>
<h1 id="how-are-git-and-github-used-together">How are Git and GitHub used together?</h1>
<p>Git and GitHub are often used together to manage codebases collaboratively. Developers create a local Git repository on their machine, make changes, and commit them to the repository&apos;s history. They can then push these changes to a remote Git repository hosted on GitHub, where other developers can review and contribute to the codebase.<br>
Developers can collaborate on code changes by creating branches and merging them back into the main branch once the changes are complete. They can also use pull requests to propose changes to the original repository owner and ask them to merge the changes.Conclusion<br>
Git and GitHub are essential tools for developers working collaboratively on codebases. Git provides a distributed version control system that allows developers to track changes and manage multiple versions of codebases. GitHub provides a web-based platform for hosting Git repositories and offers a suite of tools and services to help developers collaborate on code, track issues, and manage documentation. If you are interested in software development, learning Git and GitHub is a must.</p>
<h2 id="setting-up-a-git-repository">Setting up a Git repository</h2>
<p>Before we can start using Git, we need to set up a repository. A repository is a directory that contains all the files and folders for a project. To create a new repository, we can use the <code>git init</code> command. For example, let&apos;s say we want to create a new repository for a project called &quot;MyApp.&quot; We can do this by running the following command in our terminal:</p>
<pre><code class="language-bash">$ mkdir MyApp
$ cd MyApp
$ git init
</code></pre>
<p>This will create a new directory called &quot;MyApp&quot; and initialize a new Git repository inside it. We can now start adding files to our repository.</p>
<h2 id="adding-files-to-a-git-repository">Adding files to a Git repository</h2>
<p>To add files to our Git repository, we use the <code>git add</code> command. For example, let&apos;s say we want to add a file called &quot;index.html&quot; to our repository. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git add index.html
</code></pre>
<p>This will add the &quot;index.html&quot; file to our Git repository. We can now commit our changes.</p>
<h2 id="committing-changes-to-a-git-repository">Committing changes to a Git repository</h2>
<p>To commit changes to our Git repository, we use the <code>git commit</code> command. For example, let&apos;s say we made some changes to our &quot;index.html&quot; file and want to commit those changes. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git commit -m &quot;Updated index.html&quot;
</code></pre>
<p>This will commit our changes to the Git repository with a message that describes the changes we made. We can now push our changes to GitHub.</p>
<h2 id="pushing-changes-to-github">Pushing changes to GitHub</h2>
<p>To push our changes to GitHub, we need to first create a new repository on GitHub. Once we have created a new repository, we can push our changes to it using the <code>git push</code> command. For example, let&apos;s say we created a new repository on GitHub called &quot;MyApp.&quot; We can push our changes to this repository by running the following command:</p>
<pre><code class="language-bash">$ git remote add origin https://github.com/username/MyApp.git
$ git push -u origin master
</code></pre>
<p>This will push our changes to the &quot;MyApp&quot; repository on GitHub. We can now collaborate with other developers.</p>
<h2 id="collaborating-with-other-developers">Collaborating with other developers</h2>
<p>GitHub makes it easy to collaborate with other developers on a project. To collaborate with other developers, we can use the <code>git clone</code> command to clone a repository to our local machine. For example, let&apos;s say we want to clone the &quot;MyApp&quot; repository to our local machine. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git clone https://github.com/username/MyApp.git
</code></pre>
<p>This will clone the &quot;MyApp&quot; repository to our local machine. We can now make changes to the repository and push those changes back to GitHub.</p>
<h2 id="git-init">Git Init</h2>
<p>The <code>git init</code> command is used to initialize a new Git repository. This command creates a new directory called <code>.git</code> in the root of the project directory. This directory contains all the necessary files and folders for Git to track changes in the project. To use this command, navigate to the project directory in your terminal and run the following command:</p>
<pre><code class="language-bash">$ git init
</code></pre>
<h2 id="git-add">Git Add</h2>
<p>The <code>git add</code> command is used to add files to the staging area. The staging area is a temporary storage area where Git keeps track of changes before committing them to the repository. To add a file to the staging area, run the following command:</p>
<pre><code class="language-bash">$ git add &lt;file&gt;
</code></pre>
<p>For example, to add a file called <code>index.html</code> to the staging area, run the following command:</p>
<pre><code class="language-bash">$ git add index.html
</code></pre>
<h2 id="git-commit">Git Commit</h2>
<p>The <code>git commit</code> command is used to commit changes to the repository. When you commit changes, Git creates a new snapshot of the project&apos;s current state. To commit changes, run the following command:</p>
<pre><code class="language-bash">$ git commit -m &quot;Commit message&quot;
</code></pre>
<p>The <code>-m</code> flag is used to add a commit message. The commit message should be a brief description of the changes you made.</p>
<h2 id="git-status">Git Status</h2>
<p>The <code>git status</code> command is used to check the status of the repository. This command shows which files have been modified, which files are in the staging area, and which files are not tracked by Git. To check the status of the repository, run the following command:</p>
<pre><code class="language-bash">$ git status
</code></pre>
<h2 id="git-log">Git Log</h2>
<p>The <code>git log</code> command is used to view the commit history of the repository. This command shows a list of all the commits that have been made, along with their commit messages, author, and date. To view the commit history, run the following command:</p>
<pre><code class="language-bash">$ git log
</code></pre>
<h2 id="git-branch">Git Branch</h2>
<p>The <code>git branch</code> command is used to create, list, and delete branches. A branch is a separate line of development that allows you to work on different features or versions of the project simultaneously. To create a new branch, run the following command:</p>
<pre><code class="language-bash">$ git branch &lt;branch-name&gt;
</code></pre>
<p>For example, to create a new branch called <code>feature-branch</code>, run the following command:</p>
<pre><code class="language-bash">$ git branch feature-branch
</code></pre>
<p>To list all the branches in the repository, run the following command:</p>
<pre><code class="language-bash">$ git branch
</code></pre>
<p>To delete a branch, run the following command:</p>
<pre><code class="language-bash">$ git branch -d &lt;branch-name&gt;
</code></pre>
<p>For example, to delete the <code>feature-branch</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git branch -d feature-branch
</code></pre>
<h2 id="git-checkout">Git Checkout</h2>
<p>The <code>git checkout</code> command is used to switch between branches or restore files to a previous state. To switch to a different branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout &lt;branch-name&gt;
</code></pre>
<p>For example, to switch to the <code>feature-branch</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout feature-branch
</code></pre>
<p>To restore a file to a previous state, run the following command:</p>
<pre><code class="language-bash">$ git checkout &lt;commit-hash&gt; &lt;file&gt;
</code></pre>
<p>For example, to restore the <code>index.html</code> file to a previous commit, run the following command:</p>
<pre><code class="language-bash">$ git checkout 123abc index.html
</code></pre>
<h2 id="git-merge">Git Merge</h2>
<p>The <code>git merge</code> command is used to merge changes from one branch into another. To merge changes from the <code>feature-branch</code> branch into the <code>master</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout master
$ git merge feature-branch
</code></pre>
<h2 id="git-pull">Git Pull</h2>
<p>The <code>git pull</code> command is used to fetch and merge changes from a remote repository. To pull changes from the remote repository, run the following command:</p>
<pre><code class="language-bash">$ git pull
</code></pre>
<h2 id="git-push">Git Push</h2>
<p>The <code>git push</code> command is used to push changes to a remote repository. To push changes to the remote repository, run the following command:</p>
<pre><code class="language-bash">$ git push
</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>In conclusion, Git and GitHub are powerful tools that can help developers track changes in their codebase and collaborate with other developers. In this article, we explored the fundamentals of using Git and GitHub, including how to set up a repository, add files, commit changes, push changes to GitHub, and collaborate with other developers. We also explored some of the most important Git commands that every developer should know, including <code>git init</code>, <code>git add</code>, <code>git commit</code>, <code>git status</code>, <code>git log</code>, <code>git branch</code>, <code>git checkout</code>, <code>git merge</code>, <code>git pull</code>, and <code>git push</code>. By mastering these fundamentals and commands, developers can become more productive and efficient in their work.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>