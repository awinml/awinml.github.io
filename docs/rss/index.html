<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Ashwin Mathur]]></title><description><![CDATA[AI Research, NLP, Open-Source]]></description><link>https://awinml.github.io/</link><image><url>https://awinml.github.io/favicon.png</url><title>Ashwin Mathur</title><link>https://awinml.github.io/</link></image><generator>Ghost 5.49</generator><lastBuildDate>Tue, 18 Jul 2023 18:15:28 GMT</lastBuildDate><atom:link href="https://awinml.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease]]></title><description><![CDATA[One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI's GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.]]></description><link>https://awinml.github.io/llm-text-gen-api/</link><guid isPermaLink="false">649ff0cb174d833a229a9a26</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Sat, 01 Jul 2023 10:14:41 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease"><p><strong>Large language models (LLMs)</strong> are gaining popularity because of their capacity to produce text, translate between languages and produce various forms of creative content. However, one of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI&apos;s GPT-3, are only available through paid APIs.</p><p>Luckily, there is a smart way to use any LLM for free. By deploying your own LLM on an API endpoint, you can access it from anywhere in the world without having to pay any fees. In this article, we will show you how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.</p><h3 id="benefits-of-creating-your-own-text-generation-endpoints">Benefits of Creating Your Own Text Generation Endpoints</h3><ul><li><strong>It can save you money.</strong> Paid APIs can be expensive, especially if you are using a large number of requests. By deploying your own LLM, you can avoid these costs.</li><li><strong>Control over your data.</strong> When you use a paid API, you are giving the API provider access to your data. By deploying your own endpoint, you can keep your data safe and secure.</li><li><strong>Access to the latest models.</strong> By deploying your own endpoint, you can choose the LLM you wish to use.</li><li><strong>Ability to use the LLM capabilities on any device.</strong> LLMs require significant resources to run. The API endpoint enables any device connected to the internet to harness the capabilities of the LLM.</li></ul><h3 id="why-use-gradio-and-huggingface-spaces">Why use Gradio and HuggingFace Spaces?</h3><p>While there are popular cloud hosting providers like AWS and GCP, their setup process can be complex, and you often need to build your own Flask API. Furthermore, these providers lack free tiers that can handle large language models (LLMs).</p><p>Gradio is a tool that makes it easy to create interactive web apps that can be used to interact with LLMs. Huggingface Spaces is a free hosting service that allows you to deploy your machine learning apps to the web.</p><p>With the help of a Gradio app&apos;s API functionality, we can easily access the Language Model (LLM). We deploy the Gradio app using the free tier of HuggingFace Spaces.</p><p>Before we can get started on how to deploy the LLMs, let&apos;s create a new space on HuggingFace.</p><h3 id="creating-a-new-space-on-huggingface">Creating a new Space on HuggingFace</h3><p>A &quot;Space&quot; on HuggingFace is a hosting environment that can be used to host your ML app. Spaces are priced based on CPU type, and the simplest one is free!</p><p>Create a new Space by:</p><ul><li>Go to <a href="https://huggingface.co/spaces?ref=localhost">https://huggingface.co/spaces</a> and click Create new Space.<br>(You will need to sign-up for a HuggingFace Account to create the space.)</li><li>Select the MIT license if you&#x2019;re unsure.</li><li>Select Gradio as Space SDK.</li><li>Select Public since you want the API endpoint to be available at all times.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/07/falcon-api-gradio-space-mockup.png" class="kg-image" alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease" loading="lazy" width="632" height="898" srcset="https://awinml.github.io/content/images/size/w600/2023/07/falcon-api-gradio-space-mockup.png 600w, https://awinml.github.io/content/images/2023/07/falcon-api-gradio-space-mockup.png 632w"><figcaption>Creating a new Space on HuggingFace | Image by Author</figcaption></figure><h3 id="creating-the-gradio-app-to-access-the-llm">Creating the Gradio app to access the LLM</h3><p>In this article, we create two Gradio apps to access two types of LLM formats:</p><ul><li>A LLM checkpoint available on HuggingFace (the usual PyTorch model)</li><li>A CPU-optimized version of the LLM (GGML format based on LLaMA.cpp)</li></ul><p>The basic format of the app is the same for both formats:</p><ol><li>Load the model.</li><li>Create a function that accepts an input prompt and uses the model to return the generated text.</li><li>Make a Gradio interface to display the generated text and accept user input.</li></ol><h3 id="llm-from-a-huggingface-checkpoint">LLM from a HuggingFace Checkpoint:</h3><p>In this example we deploy the newly launched <a href="https://huggingface.co/tiiuae/falcon-7b-instruct?ref=localhost">Falcon</a> model using its HuggingFace checkpoint.</p><p>To create the Gradio app, make a new file called <code>app.py</code>, and add the following code.</p><p><strong><em>app.py</em></strong></p><pre><code class="language-python">import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
&#xA0; &#xA0; &quot;tiiuae/falcon-7b-instruct&quot;,
&#xA0; &#xA0; torch_dtype=torch.bfloat16,
&#xA0; &#xA0; trust_remote_code=True,
&#xA0; &#xA0; device_map=&quot;auto&quot;,
&#xA0; &#xA0; low_cpu_mem_usage=True,
)
tokenizer = AutoTokenizer.from_pretrained(&quot;tiiuae/falcon-7b-instruct&quot;)


def generate_text(input_text):
&#xA0; &#xA0; input_ids = tokenizer.encode(input_text, return_tensors=&quot;pt&quot;)
&#xA0; &#xA0; attention_mask = torch.ones(input_ids.shape)

&#xA0; &#xA0; output = model.generate(
&#xA0; &#xA0; &#xA0; &#xA0; input_ids,
&#xA0; &#xA0; &#xA0; &#xA0; attention_mask=attention_mask,
&#xA0; &#xA0; &#xA0; &#xA0; max_length=200,
&#xA0; &#xA0; &#xA0; &#xA0; do_sample=True,
&#xA0; &#xA0; &#xA0; &#xA0; top_k=10,
&#xA0; &#xA0; &#xA0; &#xA0; num_return_sequences=1,
&#xA0; &#xA0; &#xA0; &#xA0; eos_token_id=tokenizer.eos_token_id,
&#xA0; &#xA0; )

&#xA0; &#xA0; output_text = tokenizer.decode(output[0], skip_special_tokens=True)
&#xA0; &#xA0; print(output_text)

&#xA0; &#xA0; # Remove Prompt Echo from Generated Text
&#xA0; &#xA0; cleaned_output_text = output_text.replace(input_text, &quot;&quot;)
&#xA0; &#xA0; return cleaned_output_text


text_generation_interface = gr.Interface(
&#xA0; &#xA0; fn=generate_text,
&#xA0; &#xA0; inputs=[
&#xA0; &#xA0; &#xA0; &#xA0; gr.inputs.Textbox(label=&quot;Input Text&quot;),
&#xA0; &#xA0; ],
&#xA0; &#xA0; outputs=gr.inputs.Textbox(label=&quot;Generated Text&quot;),
&#xA0; &#xA0; title=&quot;Falcon-7B Instruct&quot;,
).launch()
</code></pre><p>This Python script uses a HuggingFace Transformers library to load the <a href="https://huggingface.co/tiiuae/falcon-7b-instruct?ref=localhost">tiiuae/falcon-7b-instruct</a> model. The max generation length is set to 200 tokens and the top_k sampling of tokens is set to 10. These text generation parameters can be set as per your requirement. The prompt is removed from the generated text so that the model only returns the generated text and not the prompt plus the generated text.</p><p>A <code>requirements.txt</code> file is created to specify the dependencies for the app. The following libraries are included in the file:</p><p><em><strong>requirements.txt</strong></em></p><pre><code>datasets
transformers
accelerate
einops
safetensors
</code></pre><p>The complete example can be viewed at: <a href="https://huggingface.co/spaces/awinml/falcon-7b-instruct-api?ref=localhost">https://huggingface.co/spaces/awinml/falcon-7b-instruct-api</a>.</p><p>The code for the app can be downloaded from: <a href="https://huggingface.co/spaces/awinml/falcon-7b-instruct-api/tree/main?ref=localhost">https://huggingface.co/spaces/awinml/falcon-7b-instruct-api/tree/main</a>.</p><h3 id="llm-from-a-cpu-optimized-ggml-format">LLM from a CPU-Optimized (GGML) format:</h3><p><a>LLaMA.cpp</a> is a C++ library that provides a high-performance inference engine for large language models (LLMs). It is based on the GGML (Graph Neural Network Machine Learning) library, which provides a fast and efficient way to represent and process graphs. LLAMA.cpp uses GGML to efficiently load and run LLMs, making it possible to run quick inference on large models.</p><p>In this example we load the <a href="https://lmsys.org/blog/2023-03-30-vicuna/?ref=localhost">Vicuna</a> model in GGML format and deploy it for inference. The inference time is significantly lower as compared to the model checkpoint available on HuggingFace.</p><p>To create the Gradio app, make a new file called <code>app.py</code>, and add the following code.</p><p><em><strong>app.py</strong></em></p><pre><code class="language-python">import os
import urllib.request
import gradio as gr
from llama_cpp import Llama


def download_file(file_link, filename):
&#xA0; &#xA0; # Checks if the file already exists before downloading
&#xA0; &#xA0; if not os.path.isfile(filename):
&#xA0; &#xA0; &#xA0; &#xA0; urllib.request.urlretrieve(file_link, filename)
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File downloaded successfully.&quot;)
&#xA0; &#xA0; else:
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File already exists.&quot;)


# Dowloading GGML model from HuggingFace
ggml_model_path = &quot;https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin&quot;
filename = &quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;

download_file(ggml_model_path, filename)


llm = Llama(model_path=filename, n_ctx=512, n_batch=126)


def generate_text(prompt=&quot;Who is the CEO of Apple?&quot;):
&#xA0; &#xA0; output = llm(
&#xA0; &#xA0; &#xA0; &#xA0; prompt,
&#xA0; &#xA0; &#xA0; &#xA0; max_tokens=256,
&#xA0; &#xA0; &#xA0; &#xA0; temperature=0.1,
&#xA0; &#xA0; &#xA0; &#xA0; top_p=0.5,
&#xA0; &#xA0; &#xA0; &#xA0; echo=False,
&#xA0; &#xA0; &#xA0; &#xA0; stop=[&quot;#&quot;],
&#xA0; &#xA0; )
&#xA0; &#xA0; output_text = output[&quot;choices&quot;][0][&quot;text&quot;].strip()

&#xA0; &#xA0; # Remove Prompt Echo from Generated Text
&#xA0; &#xA0; cleaned_output_text = output_text.replace(prompt, &quot;&quot;)
&#xA0; &#xA0; return cleaned_output_text


description = &quot;Vicuna-7B&quot;

examples = [
&#xA0; &#xA0; [&quot;What is the capital of France?&quot;, &quot;The capital of France is Paris.&quot;],
&#xA0; &#xA0; [
&#xA0; &#xA0; &#xA0; &#xA0; &quot;Who wrote the novel &apos;Pride and Prejudice&apos;?&quot;,
&#xA0; &#xA0; &#xA0; &#xA0; &quot;The novel &apos;Pride and Prejudice&apos; was written by Jane Austen.&quot;,
&#xA0; &#xA0; ],
&#xA0; &#xA0; [&quot;What is the square root of 64?&quot;, &quot;The square root of 64 is 8.&quot;],
]

gradio_interface = gr.Interface(
&#xA0; &#xA0; fn=generate_text,
&#xA0; &#xA0; inputs=&quot;text&quot;,
&#xA0; &#xA0; outputs=&quot;text&quot;,
&#xA0; &#xA0; examples=examples,
&#xA0; &#xA0; title=&quot;Vicuna-7B&quot;,
)
gradio_interface.launch()
</code></pre><p>The app first downloads the required GGML file, in this case the <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">Vicuna-7b-Q4.1 GGML</a>. The code checks if the file is already present before attempting to download it.</p><p>We leverage the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">python bindings for LLaMA.cpp</a> to load the model.</p><p>The context length of the model is set to 512 tokens. The maximum supported context length for the Vicuna model is 2048 tokens. A model with a smaller context length generates text much faster than a model with a larger context length. In most cases, a smaller context length is sufficient.</p><p>The number of tokens in the prompt and generated text can be checked using the free <a href="https://platform.openai.com/tokenizer?ref=localhost">Tokenizer tool by OpenAI</a></p><p>The batch size is set to 128 tokens. This helps speed up text generation over multithreaded CPUs.</p><p>The max generation length is set to 256 tokens, temperature to 0.1, and top-p sampling of tokens to 0.5. A list of tokens to stop generation is also added. These text generation parameters can be set as per your requirement.</p><blockquote>A detailed guide on how to use GGML versions of popular open-source LLMs for fast inference can be found at <a href="https://awinml.github.io/llm-ggml-python/?ref=localhost"><strong>How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide</strong></a>.</blockquote><p>A <code>requirements.txt</code> file is created to specify the dependencies for the app. The following libraries are included in the file:</p><p><strong><em>requirements.txt</em></strong></p><pre><code>llama-cpp-python==0.1.62
</code></pre><p>The complete example can be viewed at: <a href="https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api?ref=localhost">https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api</a>.</p><p>The code for the app can be downloaded from: <a href="https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api/tree/main?ref=localhost">https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api/tree/main</a>.</p><h3 id="deploying-the-gradio-app-on-huggingface-spaces">Deploying the Gradio app on HuggingFace Spaces:</h3><p>Deploying a Gradio app on HuggingFace Spaces is as simple as uploading the following files on your HuggingFace Space:</p><ul><li><code>app.py</code> - This file contains the code of the app.</li><li><code>requirements.txt</code> - This file lists the dependencies for the app.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/07/upload_files-mockup.png" class="kg-image" alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease" loading="lazy" width="1294" height="825" srcset="https://awinml.github.io/content/images/size/w600/2023/07/upload_files-mockup.png 600w, https://awinml.github.io/content/images/size/w1000/2023/07/upload_files-mockup.png 1000w, https://awinml.github.io/content/images/2023/07/upload_files-mockup.png 1294w" sizes="(min-width: 720px) 720px"><figcaption>Upload the files for the Gradio app on HuggingFace Spaces | Image by Author</figcaption></figure><p>The deployed app will expect you to pass in the input text or prompt, which it&#x2019;ll then use to generate an appropriate response.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/07/example-generation-mockup.png" class="kg-image" alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease" loading="lazy" width="1121" height="681" srcset="https://awinml.github.io/content/images/size/w600/2023/07/example-generation-mockup.png 600w, https://awinml.github.io/content/images/size/w1000/2023/07/example-generation-mockup.png 1000w, https://awinml.github.io/content/images/2023/07/example-generation-mockup.png 1121w" sizes="(min-width: 720px) 720px"><figcaption>Gradio app with the Vicuna model | Image by Author</figcaption></figure><h3 id="accessing-the-llm-as-an-api-endpoint">Accessing the LLM as an API Endpoint:</h3><p>The deployed Gradio app is already running a Prediction (Inference) API endpoint in the background.<br>The endpoint can be easily accessed through the <a href="https://www.gradio.app/guides/getting-started-with-the-python-client?ref=localhost">Gradio Python Client</a>.</p><p>At the bottom of the deployed app, you will see a link called &quot;Use via API&quot;. Click this link to view the instructions on how to call your app with the API.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/07/api-instructions-mockup.png" class="kg-image" alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease" loading="lazy" width="815" height="802" srcset="https://awinml.github.io/content/images/size/w600/2023/07/api-instructions-mockup.png 600w, https://awinml.github.io/content/images/2023/07/api-instructions-mockup.png 815w" sizes="(min-width: 720px) 720px"><figcaption>Using the API Endpoints to access the LLM | Image by Author</figcaption></figure><p>To use the API, you will need to install the Gradio Client Python library. You can do this by running the following command in your terminal:</p><pre><code class="language-bash">pip install gradio_client
</code></pre><p>Once you have installed the library, you can use any of the deployed apps for generating text similar to the OpenAI completion endpoints in the following manner:</p><pre><code class="language-python">from gradio_client import Client

# Pass the link to your HuggingFace Space here
client = Client(&quot;https://awinml-falcon-7b-instruct-api.hf.space/&quot;)

# Pass the Input Prompt to the model
result = client.predict(
&#xA0; &#xA0; &quot;What is the capital of USA?&quot;,
&#xA0; &#xA0; api_name=&quot;/predict&quot;
)
print(result)
</code></pre><p>This code will first create a Client object and pass the link to your HuggingFace Space to it. Then, it will pass the input prompt to the model and call the <code>predict()</code> method. The <code>predict()</code> method will return the generated text, which you can then print to the console.</p><!--kg-card-begin: markdown--><h2 id="latency-analysis">Latency Analysis:</h2>
<p>We measure the time taken by the endpoints to generate text based on different prompts. The testing was done on two private endpoints created based on the instructions in the post.</p>
<p>The <a href="https://huggingface.co/tiiuae/falcon-7b-instruct?ref=localhost">Falcon-7B</a> model has been loaded using <em>torch.bfloat16</em> weights using HuggingFace Transformers. The <a href="https://lmsys.org/blog/2023-03-30-vicuna/?ref=localhost">Vicuna-7B</a> model has been loaded using the <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin?ref=localhost">Q4.1 GGML</a> file using Llama.cpp. The model has been loaded with a context length of 512 tokens instead of the full 2048 tokens.</p>
<p>For both the models, the max generation tokens was set to 256 tokens.The temperature, top_p and top_k values were chosen based on the article and were not the same for both the models. This post is not to compare the text generation performance of both the models.</p>
<p><strong>The following prompts were used to test the latencies of the endpoints:</strong></p>
<ul>
<li>What is the capital of Greece?</li>
<li>Can you help me write a formal email to a potential business partner proposing a joint venture?</li>
<li>What are the two main differences between Python and JavaScript programming languages?</li>
<li>Extract five important keywords from the following text: How can governments utilize fiscal and monetary policies to combat economic recessions?</li>
</ul>
<h4 id="vicuna-7b">Vicuna-7B:</h4>
<ul>
<li>
<p><strong>Question: What is the capital of Greece?</strong><br>
<em>Generated Text: The capital of Greece is Athens.</em><br>
<strong>Time taken: 6.1s.</strong><br>
<br></p>
</li>
<li>
<p><strong>Question: Can you help me write a formal email to a potential business partner proposing a joint venture?</strong><br>
<em>Generated Text:<br>
I am writing to express my interest in forming a joint venture with your company. I believe that our complementary strengths and shared vision for the future make this an ideal opportunity for both of us. Our company has extensive experience in [insert relevant industry or field], and we have developed a unique product line that is well-positioned to meet the needs of our customers. We are confident that your company&apos;s expertise in [insert relevant area] would be a valuable asset to our partnership, and we believe that together we could create something truly special. I am particularly excited about the potential for collaboration on [insert specific project or initiative], which I believe has the potential to revolutionize the industry. Our teams have already begun discussing the details of this project, and we are confident that it will be a huge success. In addition to our shared goals and complementary strengths, I am also impressed by your company&apos;s commitment to [insert relevant social or environmental issue]. As someone who shares these values, I believe that our partnership could make a real difference in the world.</em><br>
<strong>Time Taken: 126.2s</strong><br>
<br></p>
</li>
<li>
<p><strong>Question: What are the two main differences between Python and JavaScript programming languages?</strong><br>
<em>Generated Text:<br>
Python is a general-purpose, high-level language that is easy to learn and use. It has a syntax that is similar to English and supports multiple data types such as integers, strings, and lists. JavaScript, on the other hand, is a client-side scripting language used for web development. It is primarily used to add interactive elements to websites.<br>
Python is an interpreted language while JavaScript is compiled. Python code is executed by an interpreter, which converts the code into machine code that can be run directly by the computer&apos;s processor. In contrast, JavaScript code is first compiled into bytecode and then run by a JavaScript engine.</em><br>
<strong>Time Taken: 72.4s</strong><br>
<br></p>
</li>
<li>
<p><strong>Question: Make a list of the top three Morgan Freeman movies. Only specify the movie names.</strong><br>
<em>Generated Text:</em><br>
<em>1. The Shawshank Redemption (1994)</em><br>
<em>2. Driving Miss Daisy (1989)</em><br>
<em>3. Glory (1989)</em><br>
<strong>Time Taken: 16.1s</strong></p>
</li>
</ul>
<h4 id="falcon-7b">Falcon-7B:</h4>
<ul>
<li><strong>Question: What is the capital of Greece?</strong><br>
<em>Generated Text: Athens is the capital of Greece.</em><br>
<strong>Time taken: 61.2s.</strong></li>
</ul>
<br>
<ul>
<li><strong>Question: Can you help me write a formal email to a potential business partner proposing a joint venture?</strong><br>
<em>Generated Text:<br>
--------- Did not capture generation. Took too long! ----------</em><br>
<strong>Time Taken: 2650s+</strong></li>
</ul>
<br>
<ul>
<li><strong>Question: What are the two main differences between Python and JavaScript programming languages?</strong><br>
<em>Generated Text:</em><br>
<em>1. Syntactical differences: Python is a higher-level language and has a more structured syntax compared to JavaScript, which is a lower-level language with less strict syntax rules.</em><br>
<em>2. Use of brackets for function calls: While Python uses the &apos;def&apos; keyword to define functions, JavaScript functions are defined using brackets or named function literals.</em><br>
<strong>Time Taken: 1524.3s</strong></li>
</ul>
<br>
<ul>
<li><strong>Question: Make a list of the top three Morgan Freeman movies. Only specify the movie names.</strong><br>
<em>Generated Text:</em><br>
<em>1. The Shawshank Redemption</em><br>
<em>2. Forrest Gump</em><br>
<em>3. Invincible</em><br>
<strong>Time Taken: 371.3s</strong></li>
</ul>
<h4 id="verdict">Verdict:</h4>
<p><strong>The Falcon-7B model</strong> is extremely slow to generate text, with the lowest latency being 61 seconds and the highest being over 2,650 seconds. This is clearly not very useful.</p>
<p>I suspect that this is because HuggingFace recently added support for Falcon to Transformers and has not yet optimized the model implementation. This could be because the architecture of the model is still largely unknown. The HuggingFace implementation also relies on external code to run the model, which may be a bottleneck. We may have better luck with other, more mature generative models on HuggingFace.</p>
<p><strong>The Vicuna GGML model</strong>, on the other hand, seems to perform extremely well, with latencies ranging from a mere 6 seconds to 126 seconds for the longest generation.</p>
<p>Llama.cpp is being constantly improved, and using a smaller, quantized version may be able to reduce this latency even further. The LLM loading parameters also significantly affect the performance, so optimizing those may also lead to some speedup.</p>
<p>The good thing is that HuggingFace provides no restrictions on the number of spaces that a user can create. This means that multiple spaces can be created and easily used to process requests in parallel.</p>
<blockquote>
<p><strong>Based on this, it is quite easy to recommend creating endpoints using the Vicuna GGML model and using it for prototyping applications instead of the expensive OpenAI GPT-3 API.</strong></p>
</blockquote>
<!--kg-card-end: markdown--><h2 id="conclusion">Conclusion</h2><p>Now, you can deploy any large language model (LLM) as an API endpoint with just a few lines of code, thanks to Gradio and HuggingFace Spaces. These tools make it simple to build your own free text generation endpoints. By deploying your own LLM on an API endpoint, you can save money by avoiding costly paid APIs while still benefiting from the remarkable capabilities of these powerful language models.</p>]]></content:encoded></item><item><title><![CDATA[How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide]]></title><description><![CDATA[Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.]]></description><link>https://awinml.github.io/llm-ggml-python/</link><guid isPermaLink="false">649f3b71e27a34833e9d3930</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Fri, 30 Jun 2023 20:38:39 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide"><p>Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. There have been several advancements like the support for 4-bit and 8-bit loading of models on HuggingFace. But they require a GPU to work. This has limited their use to people with access to specialized hardware, such as GPUs. Even though it is possible to run these LLMs on CPUs, the performance is limited and hence restricts the usage of these models.</p>
<p>Recent work by <a href="https://github.com/ggerganov?ref=localhost">Georgi Gerganov</a> has made it possible to run LLMs on CPUs with high performance. This is thanks to his implementation of the <a href="https://github.com/ggerganov/llama.cpp?ref=localhost">llama.cpp</a> library, which provides high-speed inference for a variety of LLMs.</p>
<p>The original llama.cpp library focuses on running the models locally in a shell. This does not offer a lot of flexibility to the user and makes it hard for the user to leverage the vast range of python libraries to build applications. Recently LLM frameworks like <a href="https://python.langchain.com/?ref=localhost">LangChain</a> have added support for llama.cpp using the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">llama-cpp-python</a> package.</p>
<p>In this blog post, we will see how to use the llama.cpp library in Python using the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">llama-cpp-python</a> package. This package provides Python bindings for llama.cpp, which makes it easy to use the library in Python.</p>
<p>We will also see how to use the llama-cpp-python library to run the <a href="https://lmsys.org/blog/2023-03-30-vicuna/?ref=localhost">Vicuna &#xA0;LLM</a>, which is an open-source model based on the <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/?ref=localhost">LLaMA</a> architecture that behaves like ChatGPT.</p>
<h2 id="set-up-llama-cpp-python">Set up llama-cpp-python</h2>
<p>Setting up the python bindings is as simple as running the following command:</p>
<pre><code class="language-bash">pip install llama-cpp-python
</code></pre>
<p>For more detailed installation instructions, please see the llama-cpp-python documentation: <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost#installation-from-pypi-recommended">https://github.com/abetlen/llama-cpp-python#installation-from-pypi-recommended</a>.</p>
<h2 id="using-a-llm-with-llama-cpp-python">Using a LLM with llama-cpp-python</h2>
<p>Once you have installed the llama-cpp-python package, you can start using it to run LLMs.</p>
<p>You can use any language model with llama.cpp provided that it has been converted to the GGML format. There are already GGML versions available for most popular LLMs and the required GGML can be easily found on HuggingFace.</p>
<p>An important thing to note is that the original LLMs have been quantized when converting them to GGML format. This helps reduce the memory requirement for running these large models, without a significant loss in performance. For example, this helps us load a 7 billion parameter model of size 13GB in less than 4GB of RAM.</p>
<p>In this article we use the GGML version of Vicuna-7B which is available on the Hugging Face Hub.</p>
<p>The model can be downloaded from here: <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized</a>.</p>
<h3 id="downloading-the-ggml-file-and-loading-the-llm">Downloading the GGML file and Loading the LLM</h3>
<p>The following code can be used to download the model. The code downloads the required GGML file, in this case the <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">Vicuna-7b-Q4.1 GGML</a>, from the Hugging Face Hub. The code also checks if the file is already present before attempting to download it.</p>
<pre><code class="language-python">import os
import urllib.request


def download_file(file_link, filename):
&#xA0; &#xA0; # Checks if the file already exists before downloading
&#xA0; &#xA0; if not os.path.isfile(filename):
&#xA0; &#xA0; &#xA0; &#xA0; urllib.request.urlretrieve(file_link, filename)
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File downloaded successfully.&quot;)
&#xA0; &#xA0; else:
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File already exists.&quot;)

# Dowloading GGML model from HuggingFace
ggml_model_path = &quot;https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin&quot;
filename = &quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;

download_file(ggml_model_path, filename)
</code></pre>
<p>The next step is to load the model that you want to use. This can be done using the following code:</p>
<pre><code class="language-python">from llama_cpp import Llama

llm = Llama(model_path=&quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;, n_ctx=512, n_batch=126)
</code></pre>
<p>There are two important parameters that should be set when loading the model.</p>
<ul>
<li><code>n_ctx</code>: This is used to set the maximum context size of the model. The default value is 512 tokens.</li>
</ul>
<p>The context size is the sum of the number of tokens in the input prompt and the max number of tokens that can be generated by the model. A model with smaller context size generates text much quicker than a model with a larger context size. If the use case does not demand very long generations or prompts, it is better to reduce the context length for better performance.</p>
<p>The number of tokens in the prompt and generated text can be checked using the free <a href="https://platform.openai.com/tokenizer?ref=localhost">Tokenizer tool by OpenAI</a>.</p>
<ul>
<li><code>n_batch</code>: This is used to set the maximum number of prompt tokens to batch together when generating the text. The default value is 512 tokens.</li>
</ul>
<p>The <code>n_batch</code> parameter should be set carefully. Lowering the <code>n_batch</code> helps speed up text generation over multithreaded CPUs. Reducing it too much may cause the text generation to deteriorate significantly.</p>
<p>The complete list of parameters can be viewed here: <a href="https://llama-cpp-python.readthedocs.io/en/latest/api-reference/?ref=localhost#llama_cpp.Llama">https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama</a></p>
<h3 id="generating-text-using-the-llm">Generating Text using the LLM</h3>
<p>The following code writes a simple wrapper function to generate text using the LLM.</p>
<pre><code class="language-python">def generate_text(
&#xA0; &#xA0; prompt=&quot;Who is the CEO of Apple?&quot;,
&#xA0; &#xA0; max_tokens=256,
&#xA0; &#xA0; temperature=0.1,
&#xA0; &#xA0; top_p=0.5,
&#xA0; &#xA0; echo=False,
&#xA0; &#xA0; stop=[&quot;#&quot;],
):
&#xA0; &#xA0; output = llm(
&#xA0; &#xA0; &#xA0; &#xA0; prompt,
&#xA0; &#xA0; &#xA0; &#xA0; max_tokens=max_tokens,
&#xA0; &#xA0; &#xA0; &#xA0; temperature=temperature,
&#xA0; &#xA0; &#xA0; &#xA0; top_p=top_p,
&#xA0; &#xA0; &#xA0; &#xA0; echo=echo,
&#xA0; &#xA0; &#xA0; &#xA0; stop=stop,
&#xA0; &#xA0; )
&#xA0; &#xA0; output_text = output[&quot;choices&quot;][0][&quot;text&quot;].strip()
&#xA0; &#xA0; return output_text
</code></pre>
<p>The <code>llm</code> object has several important parameters that are used while generating text:</p>
<ul>
<li>
<p><code>prompt</code>: The input prompt to the model. This text is tokenized and passed to the model.</p>
</li>
<li>
<p><code>max_tokens</code>: The parameter is used to set the maximum number of tokens the model can generate. This parameter controls the length of text generation. Default value is 128 tokens.</p>
</li>
<li>
<p><code>temperature</code>: The token sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default value is 1.</p>
</li>
<li>
<p><code>top_p</code>: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</p>
</li>
<li>
<p><code>echo</code>: Boolean parameter to control whether the model returns (echoes) the model prompt at the beginning of the generated text.</p>
</li>
<li>
<p><code>stop</code>: A list of strings that is used to stop text generation. If the model encounters any of the strings, the text generation will be stopped at that token. Used to control model hallucination and prevent the model from generating unnecessary text.</p>
</li>
</ul>
<p>The <code>llm</code> object returns a dictionary object of the form:</p>
<pre><code class="language-python">{
&#xA0; &quot;id&quot;: &quot;xxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;, &#xA0;# text generation id 
&#xA0; &quot;object&quot;: &quot;text_completion&quot;, &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;# object name
&#xA0; &quot;created&quot;: 1679561337, &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0; &#xA0;# time stamp
&#xA0; &quot;model&quot;: &quot;./models/7B/ggml-model.bin&quot;, &#xA0; &#xA0;# model path
&#xA0; &quot;choices&quot;: [
&#xA0; &#xA0; {
&#xA0; &#xA0; &#xA0; &quot;text&quot;: &quot;Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.&quot;, # generated text
&#xA0; &#xA0; &#xA0; &quot;index&quot;: 0,
&#xA0; &#xA0; &#xA0; &quot;logprobs&quot;: None,
&#xA0; &#xA0; &#xA0; &quot;finish_reason&quot;: &quot;stop&quot;
&#xA0; &#xA0; }
&#xA0; ],
&#xA0; &quot;usage&quot;: {
&#xA0; &#xA0; &quot;prompt_tokens&quot;: 14, &#xA0; &#xA0; &#xA0; # Number of tokens present in the prompt
&#xA0; &#xA0; &quot;completion_tokens&quot;: 28, &#xA0; # Number of tokens present in the generated text
&#xA0; &#xA0; &quot;total_tokens&quot;: 42
&#xA0; }
}
</code></pre>
<p>The generated text can be easily extracted from the dictionary object using <code>output[&quot;choices&quot;][0][&quot;text&quot;]</code>.</p>
<h3 id="example-text-generation-using-vicuna-7b">Example text generation using Vicuna-7B</h3>
<pre><code class="language-python">import os
import urllib.request
from llama_cpp import Llama


def download_file(file_link, filename):
&#xA0; &#xA0; # Checks if the file already exists before downloading
&#xA0; &#xA0; if not os.path.isfile(filename):
&#xA0; &#xA0; &#xA0; &#xA0; urllib.request.urlretrieve(file_link, filename)
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File downloaded successfully.&quot;)
&#xA0; &#xA0; else:
&#xA0; &#xA0; &#xA0; &#xA0; print(&quot;File already exists.&quot;)


# Dowloading GGML model from HuggingFace
ggml_model_path = &quot;https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin&quot;
filename = &quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;

download_file(ggml_model_path, filename)


llm = Llama(model_path=&quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;, n_ctx=512, n_batch=126)


def generate_text(
&#xA0; &#xA0; prompt=&quot;Who is the CEO of Apple?&quot;,
&#xA0; &#xA0; max_tokens=256,
&#xA0; &#xA0; temperature=0.1,
&#xA0; &#xA0; top_p=0.5,
&#xA0; &#xA0; echo=False,
&#xA0; &#xA0; stop=[&quot;#&quot;],
):
&#xA0; &#xA0; output = llm(
&#xA0; &#xA0; &#xA0; &#xA0; prompt,
&#xA0; &#xA0; &#xA0; &#xA0; max_tokens=max_tokens,
&#xA0; &#xA0; &#xA0; &#xA0; temperature=temperature,
&#xA0; &#xA0; &#xA0; &#xA0; top_p=top_p,
&#xA0; &#xA0; &#xA0; &#xA0; echo=echo,
&#xA0; &#xA0; &#xA0; &#xA0; stop=stop,
&#xA0; &#xA0; )
&#xA0; &#xA0; output_text = output[&quot;choices&quot;][0][&quot;text&quot;].strip()
&#xA0; &#xA0; return output_text


generate_text(
&#xA0; &#xA0; &quot;Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.&quot;,
&#xA0; &#xA0; max_tokens=356,
)
</code></pre>
<p>Generated text:</p>
<pre><code>Hawaii is a state located in the United States of America that is known for its beautiful beaches, lush landscapes, and rich culture. It is made up of six islands: Oahu, Maui, Kauai, Lanai, Molokai, and Hawaii (also known as the Big Island). Each island has its own unique attractions and experiences to offer visitors.
One of the most interesting cultural experiences in Hawaii is visiting a traditional Hawaiian village or ahupuaa. An ahupuaa is a system of land use that was used by ancient Hawaiians to manage their resources sustainably. It consists of a coastal area, a freshwater stream, and the surrounding uplands and forests. Visitors can learn about this traditional way of life at the Polynesian Cultural Center in Oahu or by visiting a traditional Hawaiian village on one of the other islands.
Another must-see attraction in Hawaii is the Pearl Harbor Memorial. This historic site commemorates the attack on Pearl Harbor on December 7, 1941, which led to the United States&apos; entry into World War II. Visitors can see the USS Arizona Memorial, a memorial that sits above the sunken battleship USS Arizona and provides an overview of the attack. They can also visit other museums and exhibits on the site to learn more about this important event in American history.
Hawaii is also known for its beautiful beaches and crystal clear waters, which are perfect for swimming, snorkeling, and sunbathing.
</code></pre>
<p>The notebook with the example can be viewed <a href="https://nbviewer.org/github/awinml/llama-cpp-python-bindings/blob/main/vicuna-7b-example-ggml.ipynb?ref=localhost">here</a>.</p>
<p>The complete code for running the examples can be found on <a href="https://github.com/awinml/llama-cpp-python-bindings?ref=localhost">GitHub</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this blog post, we explored how to use the llama.cpp library in Python with the llama-cpp-python package. These tools enable high-performance CPU-based execution of LLMs.</p>
<p>llama.cpp is updated almost every day. The speed of inference is getting better, and the community regularly adds support for new models. You can also convert your own Pytorch language models into the ggml format. llama.cpp has a &#x201C;convert.py&#x201D; that will do that for you.</p>
<p>The llama.cpp library and llama-cpp-python package provide robust solutions for running LLMs efficiently on CPUs. If you&apos;re interested in incorporating LLMs into your applications, I recommend exploring these resources.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[Machine Learning Model Deployment: From Jupyter Notebook to the Cloud]]></title><description><![CDATA[Building a machine learning model is only half the battle. Deploying the model into a production environment where it can be used to make predictions is equally important. In this article, we will explore the steps involved in deploying a machine learning model from a Jupyter Notebook to the cloud.]]></description><link>https://awinml.github.io/ml-model-deployment/</link><guid isPermaLink="false">648d7155fdfec6286dfa89d9</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Sat, 17 Jun 2023 08:46:10 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Machine Learning Model Deployment: From Jupyter Notebook to the Cloud"><p>Machine learning has become an essential tool for businesses and organizations to make data-driven decisions. However, building a machine learning model is only half the battle. Deploying the model into a production environment where it can be used to make predictions is equally important. In this article, we will explore the steps involved in deploying a machine learning model from a Jupyter Notebook to the cloud.</p><h2 id="developing-the-model">Developing the Model</h2><p>The first step in deploying a machine learning model is to develop the model itself. This is typically done in a Jupyter Notebook, where data scientists can test various modeling strategies and fine-tune the model&apos;s parameters. Once the model is developed and tested, it&apos;s time to move on to the next step.</p><h2 id="migrating-the-code">Migrating the Code</h2><p>The second step in deploying a machine learning model is to migrate the code from the Jupyter Notebook into executable modules. The idea here is to have a way to automate the whole model building and prediction process, which cannot be done on Jupyter. This involves creating preprocess, train, and inference Python scripts.</p><pre><code class="language-python"># Example of a preprocess script using scikit-learn pipelines
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

preprocess_pipeline = Pipeline([
    (&apos;imputer&apos;, SimpleImputer(strategy=&apos;median&apos;)),
    (&apos;scaler&apos;, StandardScaler())
])

preprocessed_data = preprocess_pipeline.fit_transform(data)
</code></pre><pre><code class="language-python"># Example of a train script using scikit-learn pipelines
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

train_pipeline = Pipeline([
    (&apos;preprocess&apos;, preprocess_pipeline),
    (&apos;classifier&apos;, RandomForestClassifier())
])

trained_model = train_pipeline.fit(X_train, y_train)
</code></pre><pre><code class="language-python"># Example of an inference script
def predict(model, data):
    # Model prediction code here
    preprocessed_data = preprocess_pipeline.transform(data)
    prediction = model.predict(preprocessed_data)
    return prediction
</code></pre><h2 id="saving-the-model-pipelines">Saving the Model Pipelines</h2><p>It is important to save the model pipelines after they have been trained. This is because the pipelines contain the preprocessing steps and the trained model, which are required for making predictions on new data. Saving the pipelines also allows for easy reusability of the model in other applications.</p><pre><code class="language-python"># Example of saving the trained model pipeline
import joblib

joblib.dump(train_pipeline, &apos;trained_model_pipeline.joblib&apos;)
</code></pre><pre><code class="language-python"># Example of loading the trained model pipeline
import joblib

trained_model_pipeline = joblib.load(&apos;trained_model_pipeline.joblib&apos;)
</code></pre><h2 id="building-an-api">Building an API</h2><p>The third step in deploying a machine learning model is to build an API that will take real-time inputs, usually in the form of JSON. This API could be user-facing, such as a mobile or web app, or an internal API that connects to another application to fetch the data.</p><pre><code class="language-python"># Example of a Flask API endpoint
@app.route(&apos;/predict&apos;, methods=[&apos;POST&apos;])
def predict():
    data = request.get_json()
    prediction = predict(trained_model, data)
    return jsonify(prediction.tolist())
</code></pre><h2 id="building-a-model-prediction-endpoint">Building a Model Prediction Endpoint</h2><p>The fourth step in deploying a machine learning model is to build a model prediction endpoint on the API that invokes the model&apos;s prediction API, something like the common <code>.predict()</code> or <code>.generate()</code> methods. The way the prediction works is when the input hits the <code>/predict</code> API (prediction endpoint), the inputs are passed to the model, and the API retrieves the model prediction (also usually in the form of JSON) like this: <code>{id: 0001, prediction_score: 55}</code>.</p><pre><code class="language-python"># Example of a model prediction endpoint
class ModelPredictionEndpoint:
    def __init__(self, model):
        self.model = model

    def predict(self, data):
        preprocessed_data = preprocess_pipeline.transform(data)
        prediction = self.model.predict(preprocessed_data)
        return prediction.tolist()
</code></pre><h2 id="wrapping-the-api">Wrapping the API</h2><p>The fifth step in deploying a machine learning model is to wrap the API using a container service like Docker so that the code is agnostic to the environment. This ensures that the API can be deployed on any platform without any issues.</p><pre><code class="language-dockerfile"># Example of a Dockerfile
FROM python:3.8-slim-buster
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre><h2 id="docker-and-kubernetes">Docker and Kubernetes</h2><p>Docker and Kubernetes are two popular tools used for deploying machine learning models to the cloud. Docker is a containerization platform that allows developers to package their applications and dependencies into a single container that can be run on any platform. Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications.</p><p>Using Docker and Kubernetes can simplify the deployment process and make it more scalable. Instead of deploying the application on a single server, Docker and Kubernetes allow for the deployment of multiple instances of the application, which can be scaled up or down based on demand.</p><pre><code class="language-dockerfile"># Example of a Dockerfile for a machine learning model
FROM python:3.8-slim-buster
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD [&quot;python&quot;, &quot;app.py&quot;]
</code></pre><pre><code class="language-yaml"># Example of a Kubernetes deployment file for a machine learning model
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: my-registry/my-app:latest
        ports:
        - containerPort: 5000
</code></pre><h2 id="deploying-to-the-cloud">Deploying to the Cloud</h2><p>With the model API container, you can now push this same API to any cloud provider and their service offerings: AWS EC2, Sagemaker, Beanstalk, GCP Vertex AI, App Engine. Here&apos;s an example of deploying the Docker container on AWS Elastic Beanstalk:</p><ol><li>Create an Elastic Beanstalk environment with a Docker platform.</li><li>Upload the Docker image to a container registry like Docker Hub or Amazon ECR.</li><li>Configure the Elastic Beanstalk environment to use the Docker image.</li><li>Deploy the application to the Elastic Beanstalk environment.</li></ol><pre><code class="language-bash"># Example of deploying the Docker container on AWS Elastic Beanstalk
# Build the Docker image
docker build -t my-app .

# Tag the Docker image
docker tag my-app:latest my-registry/my-app:latest

# Push the Docker image to the container registry
docker push my-registry/my-app:latest

# Create an Elastic Beanstalk environment with a Docker platform
eb create my-environment --platform &quot;Docker 20.10.7&quot;

# Configure the Elastic Beanstalk environment to use the Docker image
eb setenv DOCKER_IMAGE=my-registry/my-app:latest

# Deploy the application to the Elastic Beanstalk environment
eb deploy
</code></pre><h2 id="conclusion">Conclusion</h2><p>In conclusion, deploying a machine learning model to the cloud involves several steps, including developing the model, migrating the code, building an API, building a model prediction endpoint, wrapping the API using a container service like Docker, and deploying to the cloud. Saving the model pipelines and using Docker and Kubernetes can simplify the deployment process and make it more scalable.</p>]]></content:encoded></item><item><title><![CDATA[Financial Dashboard for Market Intelligence]]></title><description><![CDATA[Built a end-to-end financial dashboard that collects and consolidates all of a business's critical observations in one place using the information obtained from the annual 10-K SEC Filings of 12 companies.]]></description><link>https://awinml.github.io/financial-dashboard-for-market-intelligence/</link><guid isPermaLink="false">64790593edcff63646236d5e</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:56:49 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/fin_dash_cover.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://awinml.github.io/content/images/2023/06/fin_dash_cover.png" alt="Financial Dashboard for Market Intelligence"><p>Built a end-to-end financial dashboard that collects and consolidates all of a business&apos;s critical observations in one place using the information obtained from the annual 10-K SEC Filings of 12 companies.</p>
</li>
<li>
<p>Collected text data from 10-K filings from SEC EDGAR using the <a href="https://sec-api.io/?ref=localhost">SEC ExtractorAPI</a>.</p>
</li>
<li>
<p>The filings of 12 companies spanning 5 sectors were collected for the duration of 2017 to 2021. Each filing had over 34,000 words.</p>
</li>
<li>
<p>The data was cleaned and transformed for Sentiment Analysis and Summarization. The data was manually labelled for both the tasks.</p>
</li>
<li>
<p>The <strong>RoBERTa, FinBERT and DistilBERT</strong> models were fine-tuned for sentiment analysis. The best results were obtained using the fine-tuned DistilBERT model. It achieved an <strong>Accuracy of 91.11% and an ROC-AUC Score of 0.972.</strong></p>
</li>
<li>
<p>The <strong>T5, DistilPEGASUS and DistilBART</strong> models were fine-tuned for summarization. The best results were obtained using the fine-tuned DistilBART model. It achieved an <strong>ROUGE-L Score of 67.7%.</strong></p>
</li>
<li>
<p>RAKE NLTK was used to identify important keywords from the generated summaries.</p>
</li>
<li>
<p>The Financial Dashboard was deployed as a web-app using Streamlit. It contains:</p>
<ul>
<li><strong>Insights and summaries</strong> for different sections from annual corporate filings.</li>
<li>Identification of <strong>important keywords</strong> mentioned in the report.</li>
<li><strong>Sentiment-based score</strong> that measures the company&apos;s performance over a certain time period.</li>
</ul>
</li>
</ul>
<p>The app can be viewed here: <a href="https://awinml-financial-market-intelligence-app-q6lj0g.streamlit.app/?ref=localhost">Financial Dashboard</a></p>
<!--kg-card-end: markdown--><h2 id="motivation"><strong>Motivation</strong></h2><p>In the current data driven world, it is essential to have access to the right information for impactful decision making. All publicly listed companies have to file annual reports to the government. These consolidated statements allow investors, financial analysts, business owners and other interested parties to get a complete overview of the company. Companies all over the world make key financial decisions based on annually released public filings.</p><p>These corporate filings are rife with complicated legal and financial jargon and make it practically impossible for a layman to understand. In most cases these documents have to be manually read and decoded by people with expert financial and legal understanding. The goal of this project is to develop a tool that automates this tedious procedure and makes it easier to acquire crucial financial information.</p><h2 id="data"><strong>Data</strong></h2><p>To extract the text from the SEC filing, the SEC&#x2019;s ExtractorAPI was used. The API can extract any text section from 10-Q, 10-K, and 8-K SEC filings, and returns the extracted content in cleaned and standardized text or HTML format.<br>The twelve companies for which the data has been collected as listed below organized by sector:</p><ol><li>Pharmaceutical:<br>Abbvie, Pfizer, Merck</li><li>Technology:<br>Alphabet, Meta, Microsoft</li><li>Retail:<br>Costco</li><li>Oil and Natural Gas:<br>Chevron</li><li>Food and Beverages:<br>Coca Cola, Pepsico</li></ol><p>Snapshot of the data:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/data_snap.png" class="kg-image" alt="Financial Dashboard for Market Intelligence" loading="lazy" width="1330" height="602" srcset="https://awinml.github.io/content/images/size/w600/2023/06/data_snap.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/data_snap.png 1000w, https://awinml.github.io/content/images/2023/06/data_snap.png 1330w" sizes="(min-width: 720px) 720px"><figcaption>Snapshot of the data | Image by Author</figcaption></figure><h2 id="sentiment-analysis"><strong>Sentiment Analysis</strong></h2><p>A local cross validation split was created by randomly sampling rows from the records of 12 companies across sectors like Technology, Finance, Retail and Pharma. <a href="https://github.com/awinml/financial-market-intelligence/blob/main/meta_10K.pdf?ref=localhost">A sample 10k report for Meta can be viewed here</a>.</p><p>The RoBERTa, FinBERT and DistilBERT models were fine-tuned for sentiment analysis. The best results were obtained using the fine-tuned <strong>DistilBERT</strong> model. It achieved an Accuracy of 91.11% and an ROC-AUC Score of 0.972.</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>F1</th>
<th>AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td>Roberta</td>
<td>0.662</td>
<td>0.656</td>
<td>0.628</td>
</tr>
<tr>
<td>FinBERT</td>
<td>0.746</td>
<td>0.682</td>
<td>0.721</td>
</tr>
<tr>
<td>DistilBERT</td>
<td>0.911</td>
<td>0.914</td>
<td>0.972</td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="summarization"><strong>Summarization</strong></h2><p>For the summarization task, the data of Pfizer, Costco and Meta was labeled and used. A local cross validation split was created by randomly sampling rows from the records of these companies.<br>Text summarization was carried out using these three transformers models:</p><p>The T5, DistilPEGASUS and DistilBART models were fine-tuned for summarization. The best results were obtained using the fine-tuned <strong>DistilBART</strong> model. It achieved an ROUGE-L Score of 67.7%.</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th>Model</th>
<th>ROUGUE-1</th>
<th>ROUGUE-2</th>
<th>ROUGUE-L</th>
<th>ROUGUE-LSUM</th>
</tr>
</thead>
<tbody>
<tr>
<td>T5</td>
<td>32.22</td>
<td>28.5</td>
<td>31.5</td>
<td>31.5</td>
</tr>
<tr>
<td>DistilPEGASUS</td>
<td>48.32</td>
<td>34.48</td>
<td>43.51</td>
<td>31.50</td>
</tr>
<tr>
<td>DistilBART</td>
<td>72.28</td>
<td>61.15</td>
<td>67.70</td>
<td>71</td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="identifying-important-keywords"><strong>Identifying Important Keywords</strong></h2><p>RAKE NLTK was used to identify important keywords from the generated summaries.</p><h2 id="code"><strong>Code</strong>:</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/financial-market-intelligence?ref=localhost">Financial Dashboard for Market Intelligence - Github</a>.</p>]]></content:encoded></item><item><title><![CDATA[American Express - Default Prediction]]></title><description><![CDATA[Built a classification model to predict the probability that a customer does not pay back their credit card balance (defaults) based on their monthly customer statements using the data provided by American Express.]]></description><link>https://awinml.github.io/american-express-default-prediction/</link><guid isPermaLink="false">6479043fedcff63646236d20</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:53:59 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1646992914433-de93d0d06c98?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDIwfHxhbWVyaWNhbiUyMGV4cHJlc3N8ZW58MHx8fHwxNjg4Mjg1MTU5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://images.unsplash.com/photo-1646992914433-de93d0d06c98?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDIwfHxhbWVyaWNhbiUyMGV4cHJlc3N8ZW58MHx8fHwxNjg4Mjg1MTU5fDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="American Express - Default Prediction"><p>Built a <strong>classification model</strong> to predict the probability that a customer does not pay back their credit card balance (defaults) based on their monthly customer statements using the data provided by American Express.</p>
</li>
<li>
<p>The data was particularly challenging to deal with as it had <strong>5.5 million records</strong> and <strong>191 anonymized features</strong>. <strong>122 features</strong> had more than <strong>10% missing values</strong>. The target variable had <strong>severe class imbalance</strong>.</p>
</li>
<li>
<p>Engineered new features by taking different aggregations over time which helped <strong>increase model accuracy by 12%</strong>.</p>
</li>
<li>
<p><strong>Optimized XGBoost and LightGBM Classifiers using RandomSearchCV</strong> to reach the best model.</p>
</li>
<li>
<p>A Soft-Voting Ensemble of the best performing XGBoost and LightGBM Models was used to make final predictions which yielded an <strong>Accuracy of 94.48%, an F1-Score of 96.71% and an ROC-AUC Score of 96.40%.</strong></p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="data">Data</h2><p>Credit default prediction is central to managing risk in a consumer lending business. Credit default prediction allows lenders to optimize lending decisions, which leads to a better customer experience and sound business economics.</p><p>The dataset contains profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:</p><pre><code>D_* = Delinquency variables
S_* = Spend variables
P_* = Payment variables
B_* = Balance variables
R_* = Risk variables
</code></pre><p>with the following features being categorical:</p><p>&apos;B_30&apos;, &apos;B_38&apos;, &apos;D_114&apos;, &apos;D_116&apos;, &apos;D_117&apos;, &apos;D_120&apos;, &apos;D_126&apos;, &apos;D_63&apos;, &apos;D_64&apos;, &apos;D_66&apos;, &apos;D_68&apos;</p><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/amex-default-prediction?ref=localhost">here</a>.</p><h2 id="analysis">Analysis</h2><p>The complete analysis can be viewed <a href="https://nbviewer.org/github/awinml/amex-default-classification/blob/main/amex-eda.ipynb?ref=localhost">here</a>.</p><h3 id="target-distribution">Target Distribution</h3><ul><li>In the data present we observe that <strong>25.9% of records have defaulted</strong> on their credit card payments whereas <strong>74.1% have paid</strong> their bills on time.</li><li>This distribution shows us that there is <strong>severe class imbalance</strong> present.</li></ul><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/0_target_dist.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="452" height="343"></figure><h3 id="distribution-of-number-of-defaults-per-day-for-the-first-month">Distribution of Number of Defaults per day for the first Month:</h3><p>The <strong>proportion of customers that defualt is consistent across each day</strong> in the data, with a slight <strong>weekly seasonal trend</strong> influenced by the day when the customers receive their statements.</p><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/0.1_statements_default.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="1303" height="432" srcset="https://awinml.github.io/content/images/size/w600/2023/06/0.1_statements_default.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/0.1_statements_default.png 1000w, https://awinml.github.io/content/images/2023/06/0.1_statements_default.png 1303w" sizes="(min-width: 720px) 720px"></figure><h3 id="frequency-of-customer-statements-for-the-first-month">Frequency of Customer Statements for the first month:</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/1_statements_over_time.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="725" height="458" srcset="https://awinml.github.io/content/images/size/w600/2023/06/1_statements_over_time.png 600w, https://awinml.github.io/content/images/2023/06/1_statements_over_time.png 725w" sizes="(min-width: 720px) 720px"></figure><ul><li>There is <strong>weekly seasonal pattern</strong> observed in the number of statements received per day.</li><li>As seen above this trend does not seem to be significantly affecting the proportion of default.</li></ul><h3 id="distribution-of-values-of-payment-variables">Distribution of values of Payment Variables:</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/3_distribution_payment.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="1144" height="286" srcset="https://awinml.github.io/content/images/size/w600/2023/06/3_distribution_payment.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/3_distribution_payment.png 1000w, https://awinml.github.io/content/images/2023/06/3_distribution_payment.png 1144w" sizes="(min-width: 720px) 720px"></figure><ul><li>We notice that <strong>Payment 2 is heavily negatively skewed (left skewed).</strong></li><li>Even though Payment 4 have continuous values between 0 and 1, most of the density is <strong>clustered around 0 and 1</strong>.</li><li>This tells us that there may be some <strong>Gaussian Noise present</strong>. The noise can be removed and into a binary variable.</li></ul><h3 id="correlation-of-features-with-target-variable">Correlation of Features with Target Variable:</h3><ul><li><strong>Payment 2</strong> is <strong>negatively correlated</strong> with the target with a correlation of <strong>-0.67.</strong></li><li><strong>Delinquency 48</strong> is <strong>positively correlated</strong> with the target with a correlation of <strong>0.61.</strong></li></ul><h3 id="correlation-of-payment-variables-with-target">Correlation of Payment Variables with Target</h3><figure class="kg-card kg-image-card"><img src="https://awinml.github.io/content/images/2023/06/4_payment_corr.png" class="kg-image" alt="American Express - Default Prediction" loading="lazy" width="417" height="383"></figure><ul><li>We observe that Payment 2 and Target are <strong>highly negatively correlated</strong>.</li><li>This could be probably be due to the fact that people paying their bill have a less chance of default.</li></ul><h2 id="experiments">Experiments:</h2><!--kg-card-begin: markdown--><ul>
<li>
<p>The dataset presents a significant challenge with a substantial number of missing values, making imputation impossible due to the anonymization of features and the lack of a clear rationale behind imputation. This unique constraint compels us to select models that are capable of handling missing values, pushing us to explore advanced techniques.</p>
</li>
<li>
<p>One notable characteristic of the dataset is its high cardinality, boasting an impressive 191 features. However, the presence of missing values poses limitations on the utilization of conventional dimensionality reduction techniques like Principal Component Analysis (PCA) and feature selection methods such as Recursive Feature Elimination (RFE). Thus, we must seek alternative approaches to tackle this challenge.</p>
</li>
<li>
<p>In order to overcome the limitations imposed by missing values, we employ a creative strategy: engineering new features through aggregations over the time dimension. By disregarding missing values during the aggregation process, we generate dense and informative engineered features that can be effectively utilized for modeling purposes.</p>
</li>
<li>
<p>Several prominent classification models that gracefully handle missing values have been considered for this task, including XGBoost, LightGBM, and CatBoost. Notably, these models internally incorporate imputation techniques, dynamically adapting them based on the approach that yields the greatest performance improvement. This ensures that missing values do not impede the model&apos;s ability to make accurate predictions.</p>
</li>
<li>
<p>To establish a performance baseline, an XGBoost model with default hyperparameters was trained, yielding an impressive accuracy of 78.84%, an F1-Score of 54.64%, and an ROC-AUC Score of 65.72%. These results serve as a solid starting point for further improvement.</p>
</li>
<li>
<p>Subsequently, the LightGBM model with default hyperparameters was employed, resulting in notable enhancements across performance metrics. Specifically, the LightGBM model boosted the accuracy by 1%, the F1-Score by 12%, and the ROC-AUC Score by 6%, further solidifying its effectiveness.</p>
</li>
<li>
<p>To fine-tune the XGBoost and LightGBM models, a Randomized Grid Search was conducted utilizing 5-fold cross-validation. This comprehensive search approach allowed us to explore a wide range of hyperparameter combinations efficiently.</p>
</li>
<li>
<p>Hyperparameters of the XGBoost model, such as <code>n_estimators</code>, <code>max_depth</code>, and <code>learning_rate</code>, were meticulously tuned, resulting in remarkable improvements. The accuracy was enhanced by 9%, the F1-Score by 18%, and the ROC-AUC Score by 3%, showcasing the model&apos;s capacity to capture more nuanced patterns in the data.</p>
</li>
<li>
<p>Similarly, the hyperparameters of the LightGBM model, including <code>n_estimators</code>, <code>feature_fraction</code>, and <code>learning_rate</code>, were fine-tuned through the Randomized Grid Search. This meticulous optimization process led to a marginal but meaningful accuracy improvement of 0.1%, an F1-Score boost of 6%, and an impressive 10% enhancement in the ROC-AUC Score.</p>
</li>
</ul>
<p>By utilizing advanced techniques, engineering informative features, and fine-tuning the models through extensive hyperparameter optimization, we were able to elevate the performance of both the XGBoost and LightGBM models while gracefully handling the challenges imposed by missing values and high feature cardinality.</p>
<!--kg-card-end: markdown--><h2 id="results">Results:</h2><p>A <strong>Soft Voting Classifier</strong> was used to create a ensemble of both the models and was used for generating the final predictions. It achieved an <strong>Accuracy of 94.48%, an F1-Score of 96.71% and an ROC-AUC Score of 96.40%.</strong></p><p>The results from all the models have been summarized below:</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th style="text-align: center"><strong>Model</strong></th>
<th style="text-align: center"><strong>Accuracy</strong></th>
<th style="text-align: center"><strong>F1-Score</strong></th>
<th style="text-align: center"><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><strong>XGBoost</strong> <em>(default)</em></td>
<td style="text-align: center">78.84</td>
<td style="text-align: center">54.64</td>
<td style="text-align: center">65.72</td>
</tr>
<tr>
<td style="text-align: center"><strong>LightGBM</strong> <em>(default)</em></td>
<td style="text-align: center">79.84</td>
<td style="text-align: center">62.92</td>
<td style="text-align: center">71.86</td>
</tr>
<tr>
<td style="text-align: center"><strong>XGBoost</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">88.61</td>
<td style="text-align: center">80.74</td>
<td style="text-align: center">74.96</td>
</tr>
<tr>
<td style="text-align: center"><strong>LightGBM</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">88.72</td>
<td style="text-align: center">86.42</td>
<td style="text-align: center">84.22</td>
</tr>
<tr>
<td style="text-align: center"><strong>Voting Classifier</strong> <em>(XGB + LGBM)</em></td>
<td style="text-align: center"><strong>94.48</strong></td>
<td style="text-align: center"><strong>96.72</strong></td>
<td style="text-align: center"><strong>96.40</strong></td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/amex-default-classification?ref=localhost">American Express - Default Prediction Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Generate features:</li></ol><pre><code class="language-bash">  python amex-feature-engg.py
</code></pre><ol><li>Fine-tune models:</li></ol><pre><code class="language-bash">  python amex-fine-tuning.py
</code></pre><ol><li>Generate predictions:</li></ol><pre><code class="language-bash">  python amex-final-prediction.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[H&M Personalized Product Recommendations]]></title><description><![CDATA[Built a product recommendation system to recommend products based on previous transactions, as well as from customer and product meta data using the data provided by H&M.]]></description><link>https://awinml.github.io/h-m-personalized-product-recommendations/</link><guid isPermaLink="false">6478fc81edcff63646236cd6</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:24:21 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1578983662508-41895226ebfb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDJ8fEglMjBhbmQlMjBNJTIwbmVvbiUyMHNpZ258ZW58MHx8fHwxNjg4Mjg1NzgwfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><ul>
<li>
<img src="https://images.unsplash.com/photo-1578983662508-41895226ebfb?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDJ8fEglMjBhbmQlMjBNJTIwbmVvbiUyMHNpZ258ZW58MHx8fHwxNjg4Mjg1NzgwfDA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="H&amp;M Personalized Product Recommendations"><p>Built a <strong>product recommendation system</strong> to recommend products based on previous transactions, as well as from customer and product meta data using the data provided by H&amp;M.</p>
</li>
<li>
<p>The data contains <strong>1,05,542 unique products</strong> with information on <strong>24 characteristics</strong> for each product.</p>
</li>
<li>
<p>The data contains information on <strong>13,71,980 consumers</strong> and <strong>3,17,88,324 client transactions</strong> from 2018 to 2020.</p>
</li>
<li>
<p>A custom lightweight candidate retrieval method was created using a combination of retrieval of candidates that were purchased together in the last week as well as<br>
most popular candidates based on age group.</p>
</li>
<li>
<p>The candidates were ranked using a LightGBM model based on features created using the frequency of product purchase as well as the percentage of customers that purchased that product.</p>
</li>
<li>
<p>A fine tuned recommendation system using a custom candidate retrieval method and LightGBM Ranking model was used to make final predictions which yielded an <strong>MAP@12 score of 0.345 and an overall AUC of 0.76.</strong></p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="data">Data</h2><p>The purchase history of customers across time, along with supporting metadata has been provided. The goal is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends.</p><p>Files provided:</p><ul><li>articles.csv - detailed metadata for each article_id available for purchase</li><li>customers.csv - metadata for each customer_id in dataset</li><li>transactions.csv - data consisting of the purchases each customer for each date, as well as additional information. Duplicate rows correspond to multiple purchases of the same item.</li></ul><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations?ref=localhost">Kaggle</a>.</p><h2 id="analysis">Analysis</h2><p>The complete analysis can be viewed in this <a href="https://nbviewer.org/github/awinml/hm-recsys/blob/main/hm-eda.ipynb?ref=localhost">notebook</a>.</p><h3 id="distribution-of-number-of-transactions-per-day">Distribution of number of Transactions per day:</h3><ul><li>October 2019 recorded the highest number of transactions in duration of 2018 to 2020.</li><li>There is a quarterly seasonal spike of transactions.</li><li>There tends to be a large number of transactions in the month of December every year.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/9_daily_transactions.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1189" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/9_daily_transactions.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/9_daily_transactions.png 1000w, https://awinml.github.io/content/images/2023/06/9_daily_transactions.png 1189w" sizes="(min-width: 720px) 720px"><figcaption>Distribution of number of Transactions per day | Image by Author</figcaption></figure><h3 id="distribution-of-number-of-transactions-per-day-grouped-by-sales-channel">Distribution of number of Transactions per day grouped by Sales Channel:</h3><ul><li>Sales Channel 1 has daily consistent number of transactions per day with rarely any large spikes.</li><li>Sales Channel 2 consistently outperforms Sales Channel 1 throughout 2018 to 2020.</li><li>The quarterly seasonal spike of transactions is caused by transactions through Sales Channel 2.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/10_daily_transactions_sales_channel.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="966" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/10_daily_transactions_sales_channel.png 600w, https://awinml.github.io/content/images/2023/06/10_daily_transactions_sales_channel.png 966w" sizes="(min-width: 720px) 720px"><figcaption>Distribution of number of Transactions per day grouped by Sales Channel | Image by Author</figcaption></figure><h3 id="distribution-of-number-of-unique-articles-sold-per-day-grouped-by-sales-channel">Distribution of number of unique Articles sold per day grouped by Sales Channel:</h3><ul><li>Sales Channel 1 has daily consistent number of unique Articles sold per day with rarely any spikes.</li><li>Sales Channel 2 consistently sells more unique products per day than Sales Channel 1 throughout 2018 to 2020.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/11_daily_transactions_sales_channel_unique_articles.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="959" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/11_daily_transactions_sales_channel_unique_articles.png 600w, https://awinml.github.io/content/images/2023/06/11_daily_transactions_sales_channel_unique_articles.png 959w" sizes="(min-width: 720px) 720px"><figcaption>Distribution of number of unique Articles sold per day grouped by Sales Channel | Image by Author</figcaption></figure><p>After seeing the distribution of transactions and unique articles sold per day, we get the intuition that Sales Channel 1 customers are more consistent and conservative buyers.</p><p>On the other hand, customers that use Sales Channel 2 are ready to try out new products and also purchase products only in specific months during the year.</p><h3 id="distribution-of-customers-across-age-group">Distribution of Customers across Age Group:</h3><ul><li>The highest number of customers are aged 21 years.</li><li>A large proportion of the customer demographic are young adults aged 19 to 26.</li><li>There is also a significant customer base that is aged 46 to 56 years.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/7_customer_age_distribution.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1182" height="399" srcset="https://awinml.github.io/content/images/size/w600/2023/06/7_customer_age_distribution.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/7_customer_age_distribution.png 1000w, https://awinml.github.io/content/images/2023/06/7_customer_age_distribution.png 1182w" sizes="(min-width: 720px) 720px"><figcaption>Distribution of Customers across Age Group | Image by Author</figcaption></figure><h3 id="distribution-of-customers-who-have-subscribed-for-fashion-news-alerts">Distribution of Customers who have subscribed for Fashion News Alerts:</h3><ul><li>67% of all the customers have not subscribed for Fashion News Alerts.</li><li>32% of the customers have subscribed for regular updates.</li><li>1% of the customers have subscribed for monthly updates</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/8_customers_fashion_news.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="410" height="387"><figcaption>Distribution of Customers who have subscribed for Fashion News Alerts | Image by Author</figcaption></figure><h3 id="product-groups-with-the-highest-number-of-product-types">Product Groups with the highest number of Product Types:</h3><p>The product group &apos;Accessories&apos; has the highest number of product types followed by &apos;Shoes&apos; and &apos;Upper Body Garments&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/1_product_types.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="600" height="387" srcset="https://awinml.github.io/content/images/2023/06/1_product_types.png 600w"><figcaption>Product Groups with the highest number of Product Types | Image by Author</figcaption></figure><h3 id="product-types-with-the-highest-number-of-unique-articles">Product Types with the highest number of unique articles:</h3><p>The product type &apos;Trousers&apos; has the highest number of unique articles closely followed by &apos;Dress&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/2_popular_articles.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1277" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/2_popular_articles.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/2_popular_articles.png 1000w, https://awinml.github.io/content/images/2023/06/2_popular_articles.png 1277w" sizes="(min-width: 720px) 720px"><figcaption>Product Types with the highest number of unique articles | Image by Author</figcaption></figure><h3 id="product-departments-with-the-highest-number-of-unique-articles">Product Departments with the highest number of unique articles:</h3><p>The product type &apos;Jersey&apos; has the highest number of unique articles closely followed by &apos;Knitwear&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/3_highest_articles_dept.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1045" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/3_highest_articles_dept.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/3_highest_articles_dept.png 1000w, https://awinml.github.io/content/images/2023/06/3_highest_articles_dept.png 1045w" sizes="(min-width: 720px) 720px"><figcaption>Product Departments with the highest number of unique articles | Image by Author</figcaption></figure><h3 id="product-graphical-appearance-names-with-the-highest-number-of-unique-articles">Product Graphical Appearance Names with the highest number of unique articles:</h3><p>The highest number of articles are of &apos;Solid&apos; appearance followed by &apos;All over pattern&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/4_highest_articles_name.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="1024" height="876" srcset="https://awinml.github.io/content/images/size/w600/2023/06/4_highest_articles_name.png 600w, https://awinml.github.io/content/images/size/w1000/2023/06/4_highest_articles_name.png 1000w, https://awinml.github.io/content/images/2023/06/4_highest_articles_name.png 1024w" sizes="(min-width: 720px) 720px"><figcaption>Product Graphical Appearance Names with the highest number of unique articles | Image by Author</figcaption></figure><h3 id="product-index-with-the-highest-number-of-unique-articles">Product Index with the highest number of unique articles:</h3><p>The index named &apos;Ladieswear&apos; has the highest number of unique articles closely followed by &apos;Divided&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/5_highest_articles_index_name-1.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="643" height="387" srcset="https://awinml.github.io/content/images/size/w600/2023/06/5_highest_articles_index_name-1.png 600w, https://awinml.github.io/content/images/2023/06/5_highest_articles_index_name-1.png 643w"><figcaption>Product Index with the highest number of unique articles | Image by Author</figcaption></figure><h3 id="product-colour-group-names-with-the-highest-number-of-unique-articles">Product Colour Group Names with the highest number of unique articles:</h3><p>The highest number of articles are of &apos;Black&apos; colour group followed by &apos;Dark Blue&apos; and &apos;White&apos;.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://awinml.github.io/content/images/2023/06/6_highest_articles_color_group.png" class="kg-image" alt="H&amp;M Personalized Product Recommendations" loading="lazy" width="784" height="931" srcset="https://awinml.github.io/content/images/size/w600/2023/06/6_highest_articles_color_group.png 600w, https://awinml.github.io/content/images/2023/06/6_highest_articles_color_group.png 784w" sizes="(min-width: 720px) 720px"><figcaption>Product Colour Group Names with the highest number of unique articles | Image by Author</figcaption></figure><h2 id="experiments">Experiments:</h2><p>Intuition behind the custom retrieval strategy can be found <a href="https://nbviewer.org/github/awinml/hm-recsys/blob/main/hm-similar-items-explore.ipynb?ref=localhost">here.</a></p><h3 id="candidate-retrieval">Candidate Retrieval</h3><p>A lightweight candidate retrieval method was created that was a combination of the following retrieval strategies.</p><!--kg-card-begin: markdown--><ul>
<li>
<p>Recommend Items Purchased Together in the last week:</p>
<ul>
<li>Get 5 pairs of each article that were sold in the past week.</li>
<li>Ignore any article that wasn&apos;t sold within the past week.</li>
<li>Ignored any pair purchased by less than 2 customers.</li>
</ul>
</li>
<li>
<p>Recommend Items Purchased Together in the last few weeks:</p>
<ul>
<li>The number of previous weeks was tuned.</li>
</ul>
</li>
<li>
<p>Recommend most popular items based on age group</p>
</li>
</ul>
<!--kg-card-end: markdown--><h2 id="experiments-1">Experiments:</h2><p>The custom retrieval strategy was developed based on careful consideration and intuitive understanding, which can is explored in detail in this <a href="https://nbviewer.org/github/awinml/hm-recsys/blob/main/hm-similar-items-explore.ipynb?ref=localhost">notebook</a>.</p><h3 id="candidate-retrieval-1">Candidate Retrieval</h3><p>A sophisticated candidate retrieval method was devised by combining multiple effective retrieval strategies. The following approaches were employed:</p><p><strong>Recommend Items Purchased Together in the Last Week:</strong></p><ul><li>Extracted 5 pairs of articles that were sold in the past week.</li><li>Excluded any article that was not sold within the past week.</li><li>Filtered out pairs purchased by fewer than 2 customers.</li></ul><p><strong>Recommend Items Purchased Together in the Last Few Weeks:</strong></p><ul><li>Adjusted the number of previous weeks based on empirical analysis and optimization.</li></ul><p><strong>Recommend Most Popular Items Based on Age Group:</strong></p><ul><li>Identified the most popular items considering the specific age group of the customers.</li></ul><p>This candidate retrieval method is manually designed and time-aware, incorporating valuable trend information for enhanced performance.</p><h3 id="candidate-ranking">Candidate Ranking:</h3><p>To further refine the recommendation process, a comprehensive feature creation process was undertaken for candidate ranking. The features utilized are as follows:</p><p><strong>Percentage of Customers the Pair was Based on:</strong> Calculated the proportion of customers who purchased the particular pair of items.</p><p><strong>Recency of Article Purchase:</strong> Considered how recently the article was bought by customers.</p><p><strong>Number of Times the Pair of Products was Purchased:</strong> Accounted for the frequency of purchases made for the given pair.</p><p>To rank the candidates, a powerful LightGBM ranking model was employed. The LightGBM model&apos;s hyperparameters, such as <code>n_estimators</code> and <code>num_leaves</code>, were meticulously tuned to ensure optimal performance.</p><h2 id="results">Results:</h2><p>The recommendation system, finely tuned using the custom candidate retrieval method and the advanced LightGBM Ranking model, delivered impressive results. The system achieved an outstanding MAP@12 score of 0.345, showcasing its ability to accurately recommend relevant items to users. Furthermore, the overall AUC (Area Under the Curve) reached an impressive value of 0.76, confirming the system&apos;s effectiveness in capturing user preferences and generating high-quality recommendations.</p><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/hm-recsys?ref=localhost">H&amp;M Personalized Product Recommendations Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Generate local cv:</li></ol><pre><code class="language-bash">  python hm-cv.py
</code></pre><ol><li>Fine-tune models and generate predictions:</li></ol><pre><code class="language-bash">  python hm-custom-retrieval-pred.py.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[Jigsaw - Multilingual Toxic Comment Classification]]></title><description><![CDATA[Built a multilingual text classification model to predict the probability that a comment is toxic using the data provided by Google Jigsaw.]]></description><link>https://awinml.github.io/jigsaw-multilingual-toxic-comment-classification/</link><guid isPermaLink="false">6478fb9eedcff63646236cbe</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:14:29 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1611926653458-09294b3142bf?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHNvY2lhbCUyMG1lZGlhJTIwSW5zdGFncmFtJTJDJTIwRmFjZWJvb2slMkMlMjBZb3V0dWJlJTJDJTIwVHdpdHRlciUyQyUyMFBpbnRlcmVzdHxlbnwwfHx8fDE2ODgyODMyMjZ8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<ul><li>Built a <strong>multilingual text classification model</strong> to predict the probability that a comment is toxic using the data provided by Google Jigsaw.</li><li>The data had <strong>4,35,775</strong> text comments in <strong>7 different languages</strong>. </li><li>A RNN model was used as a baseline. The <strong>BERT-Multilingual-base and XLMRoBERTa</strong> models were fine-tuned to get the best results. </li><li>The best results were obtained using the fine-tuned XLMRoberta model. It achieved an <strong>Accuracy of 96.24% and an ROC-AUC Score of 93.92%.</strong></li></ul><h2 id="data">Data</h2><img src="https://images.unsplash.com/photo-1611926653458-09294b3142bf?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=M3wxMTc3M3wwfDF8c2VhcmNofDF8fHNvY2lhbCUyMG1lZGlhJTIwSW5zdGFncmFtJTJDJTIwRmFjZWJvb2slMkMlMjBZb3V0dWJlJTJDJTIwVHdpdHRlciUyQyUyMFBpbnRlcmVzdHxlbnwwfHx8fDE2ODgyODMyMjZ8MA&amp;ixlib=rb-4.0.3&amp;q=80&amp;w=2000" alt="Jigsaw - Multilingual Toxic Comment Classification"><p>A single toxic comment can ruin an online discussion. Toxicity is anything that is rude, disrespectful, or likely to make someone leave the conversation. If we can identify these toxic contributions, we can create a safer and more collaborative internet.</p><p>The goal is to find the probability that a comment is toxic. This can be done by using machine learning algorithms to analyze the text of a comment and identify words or phrases that are associated with toxicity. The algorithm can then calculate the probability that a comment is toxic based on the number of toxic words or phrases it contains.</p><p>This information can then be used to flag toxic comments for review by human moderators. By identifying and removing toxic comments, we can help to create a more positive and productive online environment for everyone.</p><h4 id="columns-in-the-dataset">Columns in the dataset:</h4><pre><code>id - identifier within each file.
comment_text - the text of the comment to be classified.
lang - the language of the comment.
toxic - whether or not the comment is classified as toxic.
</code></pre><p>The comments are composed of multiple non-English languages and come either from Civil Comments or Wikipedia talk page edits.</p><p>The dataset can be downloaded from <a href="https://www.kaggle.com/competitions/jigsaw-multilingual-toxic-comment-classification?ref=localhost">Kaggle</a>.</p><h1 id="experiments">Experiments:</h1><h2 id="rnn">RNN:</h2><p>A baseline was created using the RNN model. An embedding layer of size 64 was used. The model was trained with an Adam optimizer, employing a learning rate of 0.001, over the course of 5 epochs. The RNN model exhibited a remarkable accuracy of 83.68%, showcasing its ability to effectively process sequential data. Furthermore, the model achieved an ROC-AUC Score of 55.72%, indicating its proficiency in distinguishing between positive and negative sentiment.</p><h2 id="bert-multilingual-base">BERT-Multilingual-base:</h2><p>To leverage the power of pre-trained transformer models, the BERT-Multilingual-base was fine-tuned on the provided dataset. With the addition of a hidden layer containing 1024 neurons, the model was primed for exceptional performance. Training was conducted using the Adam optimizer, utilizing a learning rate of 0.001 and weight decay of 1e-6, extending for 10 epochs. Impressively, the fine-tuned BERT-Multilingual-base model yielded an outstanding accuracy of 93.92%, exemplifying its capacity to grasp the nuances of multilingual sentiment analysis. Additionally, it attained an impressive ROC-AUC Score of 89.55%, substantiating its robustness in making precise predictions across diverse languages.</p><h2 id="xlm-roberta">XLM RoBERTa:</h2><p>To further elevate the accuracy and generalization of the sentiment analysis task, the XLMRoberta model was meticulously fine-tuned on the available dataset. Employing the AdamW optimizer with a learning rate of 1e-5 and weight decay of 1e-5, the model was meticulously trained over the span of 7 epochs. The XLMRoberta model showcased exceptional performance, achieving an extraordinary accuracy of 96.24%. This exemplifies its remarkable ability to capture subtle linguistic nuances and comprehend sentiment across various languages. Furthermore, the model garnered an outstanding ROC-AUC Score of 93.92%, underscoring its efficacy in discriminating between positive and negative sentiments with a high degree of confidence.</p><p>For all the models that were fine-tuned:</p><ul><li>A batch size of 64 was employed during the training process, ensuring efficient computation and effective utilization of computational resources.</li><li>Binary Cross-Entropy, a commonly adopted loss function for binary classification tasks, was utilized to train the models. It enabled accurate estimation of the dissimilarity between predicted and true sentiment labels.</li></ul><h1 id="results">Results:</h1><p>Amongst the various models experimented with, the fine-tuned XLMRoberta model emerged as the clear winner, boasting the highest performance scores. Therefore, it was chosen as the ultimate choice for generating the final predictions. The fine-tuned XLMRoberta model demonstrated unparalleled accuracy, reaching an impressive 96.24%. Furthermore, it achieved a remarkable ROC-AUC Score of 93.92%. These extraordinary results solidify the model&apos;s status as the top performer, underscoring its immense capability to discern sentiment across multiple languages with exceptional precision and reliability.</p><p>The results from all the models have been summarized below:</p><!--kg-card-begin: html--><table>
<thead>
<tr>
<th style="text-align: center"><strong>Model</strong></th>
<th style="text-align: center"><strong>Accuracy</strong></th>
<th style="text-align: center"><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center"><strong>RNN</strong></td>
<td style="text-align: center">83.68</td>
<td style="text-align: center">55.72</td>
</tr>
<tr>
<td style="text-align: center"><strong>BERT-Multilingual-base</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center">93.92</td>
<td style="text-align: center">89.55</td>
</tr>
<tr>
<td style="text-align: center"><strong>XLM RoBERTa</strong> <em>(fine-tuned)</em></td>
<td style="text-align: center"><strong>96.24</strong></td>
<td style="text-align: center"><strong>93.92</strong></td>
</tr>
</tbody>
</table><!--kg-card-end: html--><h2 id="run-locally">Run Locally</h2><p>The code to run the project can be found here: <a href="https://github.com/awinml/jigsaw-toxic-comment-clf?ref=localhost">Jigsaw - Multilingual Toxic Comment Classification Github</a>.</p><ol><li>Install required libraries:</li></ol><pre><code class="language-bash">  pip install -r requirements.txt
</code></pre><ol><li>Baseline model:</li></ol><pre><code class="language-bash">  python toxic-baseline-rnn.py
</code></pre><ol><li>Fine-tune models:</li></ol><pre><code class="language-bash">  python toxic-bertm-base.py
  python toxic-xlm-roberta.py
</code></pre><h2 id="feedback">Feedback</h2><p>If you have any feedback, please reach out to me.</p>]]></content:encoded></item><item><title><![CDATA[Sports Image Classification]]></title><description><![CDATA[Built a image classification model to predict the sport that is being represented in the image.]]></description><link>https://awinml.github.io/sports-image-classification/</link><guid isPermaLink="false">6478fa5eedcff63646236ca9</guid><category><![CDATA[Projects]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 20:10:03 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/sports_img_clf_cover.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://awinml.github.io/content/images/2023/06/sports_img_clf_cover.png" alt="Sports Image Classification"><p>Have you ever wondered how machines can recognize sports just by looking at images? Well, it&apos;s all thanks to the power of Convolutional Neural Networks (CNNs). In this article, we will explore how CNNs can be used to classify sports images and compare the performance of different CNN architectures.</p>
<h2 id="dataset">Dataset</h2>
<p>The dataset used for this project is a collection of images representing 100 different types of sports and activities. The sports range from traditional sports like &quot;archery&quot;, &quot;arm wrestling&quot;, &quot;bowling&quot;, &quot;football&quot;, &quot;water polo&quot;, &quot;weightlifting&quot; to non-traditional ones like &quot;wingsuit flying&quot; and &quot;nascar racing&quot;. The goal is to predict the correct sport based on the image.</p>
<p>The dataset consists of 13572 train, 500 test, and 500 validation images. Each image is of size 224 x 224 pixels and has been segregated into train, test, and valid directories. The dataset can be downloaded from <a href="https://www.kaggle.com/datasets/gpiosenka/sports-classification?ref=localhost">Kaggle</a>.</p>
<h2 id="experiments">Experiments</h2>
<p>To classify the sports images, we compared the performance of five different CNN architectures: a custom CNN, InceptionV3, ResNet50V2, MobileNetV2, and EfficientNetB3. For each model, we fine-tuned the pre-trained ImageNet weights and added two hidden layers of 256 and 128 neurons respectively with leaky-relu activations. Dropout layers with p=0.1 were added to prevent overfitting. The number of epochs was 50 with early stopping with a patience parameter of 2 epochs. A batch size of 32 was used for training, and Sparse Categorical Cross-Entropy was used as the loss function.</p>
<h3 id="custom-cnn">Custom CNN</h3>
<p>We started with a baseline custom CNN model with 3 convolution layers and 3 dense layers. A kernel of size 3 x 3 was used for all the convolution layers. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 47 epochs yielded an Accuracy of 56.44%, F1-Score of 48.48%, and an ROC-AUC Score of 49.46%.</strong></p>
<h3 id="inceptionv3">InceptionV3</h3>
<p>The InceptionV3 model was initialized with pre-trained ImageNet weights. Only the Dense layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 22 epochs yielded an Accuracy of 68.92%, F1-Score of 64.92%, and an ROC-AUC Score of 66.64%.</strong></p>
<h3 id="resnet50v2">ResNet50V2</h3>
<p>The ResNet50V2 model was initialized with pre-trained ImageNet weights. Only the Dense layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 16 epochs yielded an Accuracy of 72.88%, F1-Score of 70.67%, and an ROC-AUC Score of 69.72%.</strong></p>
<h3 id="mobilenetv2">MobileNetV2</h3>
<p>The MobileNetV2 model was initialized with pre-trained ImageNet weights, and all the layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 8 epochs yielded an Accuracy of 86.68%, F1-Score of 86.79%, and an ROC-AUC Score of 88.36%.</strong></p>
<h3 id="efficientnetb3">EfficientNetB3</h3>
<p>The EfficientNetB3 model was initialized with pre-trained ImageNet weights, and all the layers were fine-tuned. Training the model with an <strong>Adam optimizer with a learning rate of 0.001 for 18 epochs yielded an Accuracy of 92.72%, F1-Score of 91.76%, and an ROC-AUC Score of 96.92%.</strong></p>
<h2 id="results">Results</h2>
<p><strong>The best results were obtained using a fine-tuned EfficientNetB3 model, which achieved an Accuracy of 92.72%, F1-Score of 91.76%, and an ROC-AUC Score of 96.92%.</strong> The results from all the models have been summarized in the table below:</p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>Model</strong></th>
<th style="text-align:center"><strong>Accuracy</strong></th>
<th style="text-align:center"><strong>F1-Score</strong></th>
<th><strong>ROC-AUC Score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Custom CNN</strong></td>
<td style="text-align:center">56.44</td>
<td style="text-align:center">48.48</td>
<td>49.46</td>
</tr>
<tr>
<td style="text-align:center"><strong>InceptionV3</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">68.92</td>
<td style="text-align:center">64.92</td>
<td>66.64</td>
</tr>
<tr>
<td style="text-align:center"><strong>ResNet50V2</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">72.88</td>
<td style="text-align:center">70.67</td>
<td>69.72</td>
</tr>
<tr>
<td style="text-align:center"><strong>MobileNetV2</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">86.68</td>
<td style="text-align:center">86.79</td>
<td>88.36</td>
</tr>
<tr>
<td style="text-align:center"><strong>EfficientNetB3</strong> <em>(fine-tuned)</em></td>
<td style="text-align:center">92.72</td>
<td style="text-align:center">91.76</td>
<td>96.92</td>
</tr>
</tbody>
</table>
<h2 id="deploying-the-model">Deploying the Model</h2>
<p>A web app was made using Streamlit to make predictions for new images using the best model. The live app can be viewed <a href="https://huggingface.co/spaces/awinml/sports_classification?ref=localhost">here</a>.</p>
<h2 id="run-locally">Run Locally</h2>
<p>All the code for this project can be found in this <a href="https://github.com/awinml/sports-image-classification?ref=localhost">Github repository</a>. To run the app locally, you can follow the instructions provided in the repository.</p>
<ol>
<li>Install required libraries:<pre><code class="language-bash">  pip install -r streamlit/requirements.txt
</code></pre>
</li>
<li>Fine-tune models:<pre><code class="language-bash">  python sports-clf-custom-cnn.py
  python sports-clf-inception.py
  python sports-clf-resnet.py
  python sports-clf-mobilenet.py
  python sports-clf-efficientnet.py
</code></pre>
</li>
<li>Generate predictions:<pre><code class="language-bash">  python sports-clf-final-predictions.py
</code></pre>
</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>In conclusion, we have seen how CNNs can be used to classify sports images with high accuracy. We compared the performance of five different CNN architectures and found that the EfficientNetB3 model achieved the best results. This model can be used to classify sports images in real-time, which can be useful in various applications such as sports analytics, sports broadcasting, and sports betting.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item><item><title><![CDATA[An Introduction to GitHub and Git]]></title><description><![CDATA[Learn about the fundamentals of Git and GitHub in this introductory article, including what they are, how they work, and how they can benefit your development workflow.]]></description><link>https://awinml.github.io/an-introduction-to-github-and-git/</link><guid isPermaLink="false">6478f396edcff63646236c7d</guid><category><![CDATA[Blog]]></category><dc:creator><![CDATA[Ashwin Mathur]]></dc:creator><pubDate>Thu, 01 Jun 2023 19:45:48 GMT</pubDate><media:content url="https://awinml.github.io/content/images/2023/06/cover.jpg" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: markdown--><img src="https://awinml.github.io/content/images/2023/06/cover.jpg" alt="An Introduction to GitHub and Git"><p>Git and GitHub are two of the most popular tools used by developers worldwide. Git is a distributed version control system that allows developers to track changes in their codebase, while GitHub is a web-based platform that provides a centralized location for hosting Git repositories. In this article, we will explore the fundamentals of using Git and GitHub, including how to set up a repository, commit changes, and collaborate with other developers. We will also explore some of the most important Git commands that every developer should know.</p>
<h1 id="how-are-git-and-github-used-together">How are Git and GitHub used together?</h1>
<p>Git and GitHub are often used together to manage codebases collaboratively. Developers create a local Git repository on their machine, make changes, and commit them to the repository&apos;s history. They can then push these changes to a remote Git repository hosted on GitHub, where other developers can review and contribute to the codebase.<br>
Developers can collaborate on code changes by creating branches and merging them back into the main branch once the changes are complete. They can also use pull requests to propose changes to the original repository owner and ask them to merge the changes.Conclusion<br>
Git and GitHub are essential tools for developers working collaboratively on codebases. Git provides a distributed version control system that allows developers to track changes and manage multiple versions of codebases. GitHub provides a web-based platform for hosting Git repositories and offers a suite of tools and services to help developers collaborate on code, track issues, and manage documentation. If you are interested in software development, learning Git and GitHub is a must.</p>
<h2 id="setting-up-a-git-repository">Setting up a Git repository</h2>
<p>Before we can start using Git, we need to set up a repository. A repository is a directory that contains all the files and folders for a project. To create a new repository, we can use the <code>git init</code> command. For example, let&apos;s say we want to create a new repository for a project called &quot;MyApp.&quot; We can do this by running the following command in our terminal:</p>
<pre><code class="language-bash">$ mkdir MyApp
$ cd MyApp
$ git init
</code></pre>
<p>This will create a new directory called &quot;MyApp&quot; and initialize a new Git repository inside it. We can now start adding files to our repository.</p>
<h2 id="adding-files-to-a-git-repository">Adding files to a Git repository</h2>
<p>To add files to our Git repository, we use the <code>git add</code> command. For example, let&apos;s say we want to add a file called &quot;index.html&quot; to our repository. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git add index.html
</code></pre>
<p>This will add the &quot;index.html&quot; file to our Git repository. We can now commit our changes.</p>
<h2 id="committing-changes-to-a-git-repository">Committing changes to a Git repository</h2>
<p>To commit changes to our Git repository, we use the <code>git commit</code> command. For example, let&apos;s say we made some changes to our &quot;index.html&quot; file and want to commit those changes. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git commit -m &quot;Updated index.html&quot;
</code></pre>
<p>This will commit our changes to the Git repository with a message that describes the changes we made. We can now push our changes to GitHub.</p>
<h2 id="pushing-changes-to-github">Pushing changes to GitHub</h2>
<p>To push our changes to GitHub, we need to first create a new repository on GitHub. Once we have created a new repository, we can push our changes to it using the <code>git push</code> command. For example, let&apos;s say we created a new repository on GitHub called &quot;MyApp.&quot; We can push our changes to this repository by running the following command:</p>
<pre><code class="language-bash">$ git remote add origin https://github.com/username/MyApp.git
$ git push -u origin master
</code></pre>
<p>This will push our changes to the &quot;MyApp&quot; repository on GitHub. We can now collaborate with other developers.</p>
<h2 id="collaborating-with-other-developers">Collaborating with other developers</h2>
<p>GitHub makes it easy to collaborate with other developers on a project. To collaborate with other developers, we can use the <code>git clone</code> command to clone a repository to our local machine. For example, let&apos;s say we want to clone the &quot;MyApp&quot; repository to our local machine. We can do this by running the following command:</p>
<pre><code class="language-bash">$ git clone https://github.com/username/MyApp.git
</code></pre>
<p>This will clone the &quot;MyApp&quot; repository to our local machine. We can now make changes to the repository and push those changes back to GitHub.</p>
<h2 id="git-init">Git Init</h2>
<p>The <code>git init</code> command is used to initialize a new Git repository. This command creates a new directory called <code>.git</code> in the root of the project directory. This directory contains all the necessary files and folders for Git to track changes in the project. To use this command, navigate to the project directory in your terminal and run the following command:</p>
<pre><code class="language-bash">$ git init
</code></pre>
<h2 id="git-add">Git Add</h2>
<p>The <code>git add</code> command is used to add files to the staging area. The staging area is a temporary storage area where Git keeps track of changes before committing them to the repository. To add a file to the staging area, run the following command:</p>
<pre><code class="language-bash">$ git add &lt;file&gt;
</code></pre>
<p>For example, to add a file called <code>index.html</code> to the staging area, run the following command:</p>
<pre><code class="language-bash">$ git add index.html
</code></pre>
<h2 id="git-commit">Git Commit</h2>
<p>The <code>git commit</code> command is used to commit changes to the repository. When you commit changes, Git creates a new snapshot of the project&apos;s current state. To commit changes, run the following command:</p>
<pre><code class="language-bash">$ git commit -m &quot;Commit message&quot;
</code></pre>
<p>The <code>-m</code> flag is used to add a commit message. The commit message should be a brief description of the changes you made.</p>
<h2 id="git-status">Git Status</h2>
<p>The <code>git status</code> command is used to check the status of the repository. This command shows which files have been modified, which files are in the staging area, and which files are not tracked by Git. To check the status of the repository, run the following command:</p>
<pre><code class="language-bash">$ git status
</code></pre>
<h2 id="git-log">Git Log</h2>
<p>The <code>git log</code> command is used to view the commit history of the repository. This command shows a list of all the commits that have been made, along with their commit messages, author, and date. To view the commit history, run the following command:</p>
<pre><code class="language-bash">$ git log
</code></pre>
<h2 id="git-branch">Git Branch</h2>
<p>The <code>git branch</code> command is used to create, list, and delete branches. A branch is a separate line of development that allows you to work on different features or versions of the project simultaneously. To create a new branch, run the following command:</p>
<pre><code class="language-bash">$ git branch &lt;branch-name&gt;
</code></pre>
<p>For example, to create a new branch called <code>feature-branch</code>, run the following command:</p>
<pre><code class="language-bash">$ git branch feature-branch
</code></pre>
<p>To list all the branches in the repository, run the following command:</p>
<pre><code class="language-bash">$ git branch
</code></pre>
<p>To delete a branch, run the following command:</p>
<pre><code class="language-bash">$ git branch -d &lt;branch-name&gt;
</code></pre>
<p>For example, to delete the <code>feature-branch</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git branch -d feature-branch
</code></pre>
<h2 id="git-checkout">Git Checkout</h2>
<p>The <code>git checkout</code> command is used to switch between branches or restore files to a previous state. To switch to a different branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout &lt;branch-name&gt;
</code></pre>
<p>For example, to switch to the <code>feature-branch</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout feature-branch
</code></pre>
<p>To restore a file to a previous state, run the following command:</p>
<pre><code class="language-bash">$ git checkout &lt;commit-hash&gt; &lt;file&gt;
</code></pre>
<p>For example, to restore the <code>index.html</code> file to a previous commit, run the following command:</p>
<pre><code class="language-bash">$ git checkout 123abc index.html
</code></pre>
<h2 id="git-merge">Git Merge</h2>
<p>The <code>git merge</code> command is used to merge changes from one branch into another. To merge changes from the <code>feature-branch</code> branch into the <code>master</code> branch, run the following command:</p>
<pre><code class="language-bash">$ git checkout master
$ git merge feature-branch
</code></pre>
<h2 id="git-pull">Git Pull</h2>
<p>The <code>git pull</code> command is used to fetch and merge changes from a remote repository. To pull changes from the remote repository, run the following command:</p>
<pre><code class="language-bash">$ git pull
</code></pre>
<h2 id="git-push">Git Push</h2>
<p>The <code>git push</code> command is used to push changes to a remote repository. To push changes to the remote repository, run the following command:</p>
<pre><code class="language-bash">$ git push
</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>In conclusion, Git and GitHub are powerful tools that can help developers track changes in their codebase and collaborate with other developers. In this article, we explored the fundamentals of using Git and GitHub, including how to set up a repository, add files, commit changes, push changes to GitHub, and collaborate with other developers. We also explored some of the most important Git commands that every developer should know, including <code>git init</code>, <code>git add</code>, <code>git commit</code>, <code>git status</code>, <code>git log</code>, <code>git branch</code>, <code>git checkout</code>, <code>git merge</code>, <code>git pull</code>, and <code>git push</code>. By mastering these fundamentals and commands, developers can become more productive and efficient in their work.</p>
<!--kg-card-end: markdown-->]]></content:encoded></item></channel></rss>