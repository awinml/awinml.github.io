<!DOCTYPE html>
<html âš¡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease</title>

    <meta name="description" content="One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI&#x27;s GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.">
    <link rel="icon" href="https://awinml.github.io/content/images/size/w256h256/2023/06/android-chrome-192x192.png" type="image/png">
    <link rel="canonical" href="https://awinml.github.io/llm-text-gen-api/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    
    <meta property="og:site_name" content="Ashwin Mathur">
    <meta property="og:type" content="article">
    <meta property="og:title" content="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease">
    <meta property="og:description" content="One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI&#x27;s GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.">
    <meta property="og:url" content="https://awinml.github.io/llm-text-gen-api/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta property="article:published_time" content="2023-07-01T10:14:41.000Z">
    <meta property="article:modified_time" content="2023-07-01T10:18:10.000Z">
    <meta property="article:tag" content="Blog">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease">
    <meta name="twitter:description" content="One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI&#x27;s GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.">
    <meta name="twitter:url" content="https://awinml.github.io/llm-text-gen-api/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Ashwin Mathur">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Blog">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="1500">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Ashwin Mathur",
        "url": "https://awinml.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://awinml.github.io/content/images/size/w256h256/2023/06/android-chrome-192x192.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Ashwin Mathur",
        "url": "https://awinml.github.io/author/ashwin/",
        "sameAs": []
    },
    "headline": "No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease",
    "url": "https://awinml.github.io/llm-text-gen-api/",
    "datePublished": "2023-07-01T10:14:41.000Z",
    "dateModified": "2023-07-01T10:18:10.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&ixlib=rb-4.0.3&q=80&w=2000",
        "width": 2000,
        "height": 1500
    },
    "keywords": "Blog",
    "description": "One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI&#x27;s GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.",
    "mainEntityOfPage": "https://awinml.github.io/llm-text-gen-api/"
}
    </script>

    <meta name="generator" content="Ghost 5.49">
    <link rel="alternate" type="application/rss+xml" title="Ashwin Mathur" href="https://awinml.github.io/rss/">

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: var(--ghost-accent-color, #1292EE);
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }
    
    amp-youtube {
        height: calc(100vw / 1.78);
        width: 100vw;
        position: relative;
    }

    amp-youtube img {
        position: absolute;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: var(--ghost-accent-color, #1292EE);
    }

    .post-content blockquote.kg-blockquote-alt {
        font-size: 1.2em;
        font-style: italic;
        line-height: 1.6em;
        text-align: center;
        color: #738a94;
        padding: 0.75em 3em 1.25em;
    }

    .post-content blockquote.kg-blockquote-alt::before {
        display: none;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #15171a;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 3px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "â€¢";
        margin: 0 .5em;
    }

    .kg-toggle-card-icon {
        display: none;
    }

    .kg-toggle-content {
        margin-top: 0.8rem;
    }

    .kg-product-card-container {
        background: transparent;
        padding: 20px;
        width: 100%;
        border-radius: 5px;
        box-shadow: inset 0 0 0 1px rgb(124 139 154 / 25%);
    }

    .kg-product-card-description p {
        margin-top: 1.5em;
    }

    .kg-product-card-description ul {
        margin-left: 24px;
    }

    .kg-product-card-title {
        font-size: 1.9rem;
        font-weight: 700;
    }

    .kg-product-card-rating-star {
        height: 28px;
        width: 20px;
        margin-right: 2px;
    }

    .kg-product-card-rating-star svg {
    width: 16px;
    height: 16px;
    fill: currentColor;
    opacity: 0.15;
    }

    .kg-product-card-rating-active.kg-product-card-rating-star svg {
    opacity: 1;
    }

    .kg-nft-card-container {
        position: relative;
        display: flex;
        flex: auto;
        flex-direction: column;
        text-decoration: none;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.4rem;
        font-weight: 400;
        box-shadow: 0 2px 6px -2px rgb(0 0 0 / 10%), 0 0 1px rgb(0 0 0 / 40%);
        width: 100%;
        max-width: 512px;
        color: #15212A;
        background: #fff;
        border-radius: 5px;
        transition: none;
        margin: 0 auto;
    }

    .kg-nft-metadata {
        padding: 2.0rem;
    }

    .kg-nft-image-container {
        position: relative;
    }

    .kg-nft-image {
        display: flex;
        border-radius: 5px 5px 0 0;
    }

    .kg-nft-header {
        display: flex;
        justify-content: space-between;
        align-items: flex-start;
        gap: 20px;
    }

    .kg-nft-header h4.kg-nft-title {
        font-size: 1.9rem;
        font-weight: 700;
        margin: 0;
        color: #15212A;
    }

    .kg-nft-header amp-img {
        max-width: 114px;
        max-height: 26px;
    }

    .kg-nft-opensea-logo {
        margin-top: 2px;
        width: 100px;
    }

    .kg-nft-creator {
        font-family: inherit;
        color: #95A1AD;
    }

    .kg-nft-creator span {
        font-weight: 500;
        color: #15212A;
    }

    .kg-nft-card p.kg-nft-description {
        font-size: 1.4rem;
        line-height: 1.4em;
        margin: 2.0rem 0 0;
        color: #222;
    }

    .kg-button-card {
        display: flex;
        position: static;
        align-items: center;
        width: 100%;
        justify-content: center;
    }

    .kg-btn {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 2.0rem;
        height: 4.0rem;
        line-height: 4.0rem;
        font-size: 1.65rem;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
    }

    .kg-btn:hover {
        opacity: 0.85;
    }

    .kg-btn-accent {
        background-color: var(--ghost-accent-color, #1292EE);
        color: #fff;
    }

    .kg-callout-card {
        display: flex;
        padding: 20px 28px;
        border-radius: 3px;
    }

    .kg-callout-card-grey {
        background: rgba(124, 139, 154, 0.13);
    }

    .kg-callout-card-white {
        background: transparent;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-callout-card-blue {
        background: rgba(33, 172, 232, 0.12);
    }

    .kg-callout-card-green {
        background: rgba(52, 183, 67, 0.12);
    }

    .kg-callout-card-yellow {
        background: rgba(240, 165, 15, 0.13);
    }

    .kg-callout-card-red {
        background: rgba(209, 46, 46, 0.11);
    }

    .kg-callout-card-pink {
        background: rgba(225, 71, 174, 0.11);
    }

    .kg-callout-card-purple {
        background: rgba(135, 85, 236, 0.12);
    }

    .kg-callout-card-accent {
        background: var(--ghost-accent-color);
        color: #fff;
    }

    .kg-callout-card-accent a {
        color: #fff;
    }

    .kg-callout-emoji {
        padding-right: 16px;
        line-height: 1.3;
        font-size: 1.25em;
    }

    .kg-header-card {
        padding: 6em 3em;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        text-align: center;
    }

    .kg-header-card.kg-size-small {
        padding-top: 4em;
        padding-bottom: 4em;
    }

    .kg-header-card.kg-size-large {
        padding-top: 12em;
        padding-bottom: 12em;
    }

    .kg-header-card.kg-width-full {
        padding-left: 4em;
        padding-right: 4em;
    }

    .kg-header-card.kg-align-left {
        text-align: left;
        align-items: flex-start;
    }

    .kg-header-card.kg-style-dark {
        background: #15171a;
        color: #ffffff;
    }

    .kg-header-card.kg-style-light {
        color: #15171a;
        border: 1px solid rgba(124, 139, 154, 0.25);
        border-width: 1px 0;
    }

    .kg-header-card.kg-style-accent {
        background-color: var(--ghost-accent-color);
    }

    .kg-header-card.kg-style-image {
        background-color: #e7e7eb;
        background-size: cover;
        background-position: center center;
    }

    .kg-header-card h2 {
        font-size: 4em;
        font-weight: 700;
        line-height: 1.1em;
        margin: 0;
    }

    .kg-header-card h2 strong {
        font-weight: 800;
    }

    .kg-header-card.kg-size-small h2 {
        font-size: 3em;
    }

    .kg-header-card.kg-size-large h2 {
        font-size: 5em;
    }

    .kg-header-card h3 {
        font-size: 1.25em;
        font-weight: 500;
        line-height: 1.3em;
        margin: 0;
    }

    .kg-header-card h3 strong {
        font-weight: 600;
    }

    .kg-header-card.kg-size-small h3 {
        font-size: 1em;
    }

    .kg-header-card.kg-size-large h3 {
        font-size: 1.5em;
    }

    .kg-header-card:not(.kg-style-light) h2,
    .kg-header-card:not(.kg-style-light) h3 {
        color: #ffffff;
    }

    .kg-header-card a.kg-header-card-button {
        display: flex;
        position: static;
        align-items: center;
        padding: 0 1.2em;
        height: 2.4em;
        line-height: 1em;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-size: 0.95em;
        font-weight: 600;
        text-decoration: none;
        border-radius: 5px;
        transition: opacity 0.2s ease-in-out;
        background-color: var(--ghost-accent-color);
        color: #ffffff;
        margin: 1.75em 0 0;
    }

    .kg-header-card a.kg-header-card-button:hover {
        opacity: 0.85;
    }

    .kg-header-card.kg-size-large a.kg-header-card-button {
        margin-top: 2em;
    }

    .kg-header-card.kg-size-small a.kg-header-card-button {
        margin-top: 1.5em;
    }

    .kg-header-card.kg-style-image a.kg-header-card-button,
    .kg-header-card.kg-style-dark a.kg-header-card-button {
        background: #ffffff;
        color: #15171a;
    }

    .kg-header-card.kg-style-accent a.kg-header-card-button {
        background: #ffffff;
        color: var(--ghost-accent-color);
    }

    .kg-audio-card {
        display: flex;
        width: 100%;
        box-shadow: inset 0 0 0 1px rgba(124, 139, 154, 0.25);
    }

    .kg-audio-thumbnail {
        display: flex;
        justify-content: center;
        align-items: center;
        width: 80px;
        min-width: 80px;
        height: 80px;
        background: transparent;
        object-fit: cover;
        aspect-ratio: 1/1;
        border-radius: 3px 0 0 3px;
    }

    .kg-audio-thumbnail.placeholder {
        background: var(--ghost-accent-color);
    }

    .kg-audio-thumbnail.placeholder svg {
        width: 24px;
        height: 24px;
        fill: white;
    }

    .kg-audio-player-container {
        position: relative;
        display: flex;
        flex-direction: column;
        justify-content: space-between;
        width: 100%;
        --seek-before-width: 0%;
        --volume-before-width: 100%;
        --buffered-width: 0%;
    }

    .kg-audio-title {
        width: 100%;
        padding: 8px 12px 0;
        border: none;
        font-family: inherit;
        font-size: 1.1em;
        font-weight: 700;
        background: transparent;
    }

    .kg-audio-player {
        display: none;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }

    :root {--ghost-accent-color: #023e8a;}
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    <script async custom-element="amp-analytics" src="https://cdn.ampproject.org/v0/amp-analytics-0.1.js"></script>

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="https://awinml.github.io">
                <amp-img class="site-icon" src="https://awinml.github.io/content/images/2023/06/android-chrome-192x192.png" width="50" height="50" layout="fixed" alt="Ashwin Mathur"></amp-img>
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease</h1>
                <section class="post-meta">
                    Ashwin Mathur -
                    <time class="post-date" datetime="2023-07-01">01 Jul 2023</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000" width="600" height="340" layout="responsive" 
                alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease"
                ></amp-img>
            </figure>
            <section class="post-content">

                <p><strong>Large language models (LLMs)</strong> are gaining popularity because of their capacity to produce text, translate between languages and produce various forms of creative content. However, one of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI's GPT-3, are only available through paid APIs.</p><p>Luckily, there is a smart way to use any LLM for free. By deploying your own LLM on an API endpoint, you can access it from anywhere in the world without having to pay any fees. In this article, we will show you how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.</p><h3 id="benefits-of-creating-your-own-text-generation-endpoints">Benefits of Creating Your Own Text Generation Endpoints</h3><ul><li><strong>It can save you money.</strong> Paid APIs can be expensive, especially if you are using a large number of requests. By deploying your own LLM, you can avoid these costs.</li><li><strong>Control over your data.</strong> When you use a paid API, you are giving the API provider access to your data. By deploying your own endpoint, you can keep your data safe and secure.</li><li><strong>Access to the latest models.</strong> By deploying your own endpoint, you can choose the LLM you wish to use.</li><li><strong>Ability to use the LLM capabilities on any device.</strong> LLMs require significant resources to run. The API endpoint enables any device connected to the internet to harness the capabilities of the LLM.</li></ul><h3 id="why-use-gradio-and-huggingface-spaces">Why use Gradio and HuggingFace Spaces?</h3><p>While there are popular cloud hosting providers like AWS and GCP, their setup process can be complex, and you often need to build your own Flask API. Furthermore, these providers lack free tiers that can handle large language models (LLMs).</p><p>Gradio is a tool that makes it easy to create interactive web apps that can be used to interact with LLMs. Huggingface Spaces is a free hosting service that allows you to deploy your machine learning apps to the web.</p><p>With the help of a Gradio app's API functionality, we can easily access the Language Model (LLM). We deploy the Gradio app using the free tier of HuggingFace Spaces.</p><p>Before we can get started on how to deploy the LLMs, let's create a new space on HuggingFace.</p><h3 id="creating-a-new-space-on-huggingface">Creating a new Space on HuggingFace</h3><p>A "Space" on HuggingFace is a hosting environment that can be used to host your ML app. Spaces are priced based on CPU type, and the simplest one is free!</p><p>Create a new Space by:</p><ul><li>Go to <a href="https://huggingface.co/spaces?ref=localhost">https://huggingface.co/spaces</a> and click Create new Space.<br />(You will need to sign-up for a HuggingFace Account to create the space.)</li><li>Select the MIT license if youâ€™re unsure.</li><li>Select Gradio as Space SDK.</li><li>Select Public since you want the API endpoint to be available at all times.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Creating a new Space on HuggingFace | Image by Author</figcaption></figure><h3 id="creating-the-gradio-app-to-access-the-llm">Creating the Gradio app to access the LLM</h3><p>In this article, we create two Gradio apps to access two types of LLM formats:</p><ul><li>A LLM checkpoint available on HuggingFace (the usual PyTorch model)</li><li>A CPU-optimized version of the LLM (GGML format based on LLaMA.cpp)</li></ul><p>The basic format of the app is the same for both formats:</p><ol><li>Load the model.</li><li>Create a function that accepts an input prompt and uses the model to return the generated text.</li><li>Make a Gradio interface to display the generated text and accept user input.</li></ol><h3 id="llm-from-a-huggingface-checkpoint">LLM from a HuggingFace Checkpoint:</h3><p>In this example we deploy the newly launched <a href="https://huggingface.co/tiiuae/falcon-7b-instruct?ref=localhost">Falcon</a> model using its HuggingFace checkpoint.</p><p>To create the Gradio app, make a new file called <code>app.py</code>, and add the following code.</p><p><strong><em>app.py</em></strong></p><pre><code class="language-python">import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model = AutoModelForCausalLM.from_pretrained(
Â  Â  "tiiuae/falcon-7b-instruct",
Â  Â  torch_dtype=torch.bfloat16,
Â  Â  trust_remote_code=True,
Â  Â  device_map="auto",
Â  Â  low_cpu_mem_usage=True,
)
tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct")


def generate_text(input_text):
Â  Â  input_ids = tokenizer.encode(input_text, return_tensors="pt")
Â  Â  attention_mask = torch.ones(input_ids.shape)

Â  Â  output = model.generate(
Â  Â  Â  Â  input_ids,
Â  Â  Â  Â  attention_mask=attention_mask,
Â  Â  Â  Â  max_length=200,
Â  Â  Â  Â  do_sample=True,
Â  Â  Â  Â  top_k=10,
Â  Â  Â  Â  num_return_sequences=1,
Â  Â  Â  Â  eos_token_id=tokenizer.eos_token_id,
Â  Â  )

Â  Â  output_text = tokenizer.decode(output[0], skip_special_tokens=True)
Â  Â  print(output_text)

Â  Â  # Remove Prompt Echo from Generated Text
Â  Â  cleaned_output_text = output_text.replace(input_text, "")
Â  Â  return cleaned_output_text


text_generation_interface = gr.Interface(
Â  Â  fn=generate_text,
Â  Â  inputs=[
Â  Â  Â  Â  gr.inputs.Textbox(label="Input Text"),
Â  Â  ],
Â  Â  outputs=gr.inputs.Textbox(label="Generated Text"),
Â  Â  title="Falcon-7B Instruct",
).launch()
</code></pre><p>This Python script uses a HuggingFace Transformers library to load the <a href="https://huggingface.co/tiiuae/falcon-7b-instruct?ref=localhost">tiiuae/falcon-7b-instruct</a> model. The max generation length is set to 200 tokens and the top_k sampling of tokens is set to 10. These text generation parameters can be set as per your requirement. The prompt is removed from the generated text so that the model only returns the generated text and not the prompt plus the generated text.</p><p>A <code>requirements.txt</code> file is created to specify the dependencies for the app. The following libraries are included in the file:</p><p><em><strong>requirements.txt</strong></em></p><pre><code>datasets
transformers
accelerate
einops
safetensors
</code></pre><p>The complete example can be viewed at: <a href="https://huggingface.co/spaces/awinml/falcon-7b-instruct-api?ref=localhost">https://huggingface.co/spaces/awinml/falcon-7b-instruct-api</a>.</p><p>The code for the app can be downloaded from: <a href="https://huggingface.co/spaces/awinml/falcon-7b-instruct-api/tree/main?ref=localhost">https://huggingface.co/spaces/awinml/falcon-7b-instruct-api/tree/main</a>.</p><h3 id="llm-from-a-cpu-optimized-ggml-format">LLM from a CPU-Optimized (GGML) format:</h3><p><a>LLaMA.cpp</a> is a C++ library that provides a high-performance inference engine for large language models (LLMs). It is based on the GGML (Graph Neural Network Machine Learning) library, which provides a fast and efficient way to represent and process graphs. LLAMA.cpp uses GGML to efficiently load and run LLMs, making it possible to run quick inference on large models.</p><p>In this example we load the <a href="https://lmsys.org/blog/2023-03-30-vicuna/?ref=localhost">Vicuna</a> model in GGML format and deploy it for inference. The inference time is significantly lower as compared to the model checkpoint available on HuggingFace.</p><p>To create the Gradio app, make a new file called <code>app.py</code>, and add the following code.</p><p><em><strong>app.py</strong></em></p><pre><code class="language-python">import os
import urllib.request
import gradio as gr
from llama_cpp import Llama


def download_file(file_link, filename):
Â  Â  # Checks if the file already exists before downloading
Â  Â  if not os.path.isfile(filename):
Â  Â  Â  Â  urllib.request.urlretrieve(file_link, filename)
Â  Â  Â  Â  print("File downloaded successfully.")
Â  Â  else:
Â  Â  Â  Â  print("File already exists.")


# Dowloading GGML model from HuggingFace
ggml_model_path = "https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin"
filename = "ggml-vicuna-7b-1.1-q4_1.bin"

download_file(ggml_model_path, filename)


llm = Llama(model_path=filename, n_ctx=512, n_batch=126)


def generate_text(prompt="Who is the CEO of Apple?"):
Â  Â  output = llm(
Â  Â  Â  Â  prompt,
Â  Â  Â  Â  max_tokens=256,
Â  Â  Â  Â  temperature=0.1,
Â  Â  Â  Â  top_p=0.5,
Â  Â  Â  Â  echo=False,
Â  Â  Â  Â  stop=["#"],
Â  Â  )
Â  Â  output_text = output["choices"][0]["text"].strip()

Â  Â  # Remove Prompt Echo from Generated Text
Â  Â  cleaned_output_text = output_text.replace(prompt, "")
Â  Â  return cleaned_output_text


description = "Vicuna-7B"

examples = [
Â  Â  ["What is the capital of France?", "The capital of France is Paris."],
Â  Â  [
Â  Â  Â  Â  "Who wrote the novel 'Pride and Prejudice'?",
Â  Â  Â  Â  "The novel 'Pride and Prejudice' was written by Jane Austen.",
Â  Â  ],
Â  Â  ["What is the square root of 64?", "The square root of 64 is 8."],
]

gradio_interface = gr.Interface(
Â  Â  fn=generate_text,
Â  Â  inputs="text",
Â  Â  outputs="text",
Â  Â  examples=examples,
Â  Â  title="Vicuna-7B",
)
gradio_interface.launch()
</code></pre><p>The app first downloads the required GGML file, in this case the <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">Vicuna-7b-Q4.1 GGML</a>. The code checks if the file is already present before attempting to download it.</p><p>We leverage the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">python bindings for LLaMA.cpp</a> to load the model.</p><p>The context length of the model is set to 512 tokens. The maximum supported context length for the Vicuna model is 2048 tokens. A model with a smaller context length generates text much faster than a model with a larger context length. In most cases, a smaller context length is sufficient.</p><p>The number of tokens in the prompt and generated text can be checked using the free <a href="https://platform.openai.com/tokenizer?ref=localhost">Tokenizer tool by OpenAI</a></p><p>The batch size is set to 128 tokens. This helps speed up text generation over multithreaded CPUs.</p><p>The max generation length is set to 256 tokens, temperature to 0.1, and top-p sampling of tokens to 0.5. A list of tokens to stop generation is also added. These text generation parameters can be set as per your requirement.</p><p>A detailed guide on how to use GGML versions of popular open-source LLMs for fast inference can be found at <a href="https://awinml.github.io/llama-cpp-python?ref=localhost">https://awinml.github.io/llama-cpp-python</a>.</p><p>A <code>requirements.txt</code> file is created to specify the dependencies for the app. The following libraries are included in the file:</p><p><strong><em>requirements.txt</em></strong></p><pre><code>llama-cpp-python==0.1.62
</code></pre><p>The complete example can be viewed at: <a href="https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api?ref=localhost">https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api</a>.</p><p>The code for the app can be downloaded from: <a href="https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api/tree/main?ref=localhost">https://huggingface.co/spaces/awinml/vicuna-7b-ggml-api/tree/main</a>.</p><h3 id="deploying-the-gradio-app-on-huggingface-spaces">Deploying the Gradio app on HuggingFace Spaces:</h3><p>Deploying a Gradio app on HuggingFace Spaces is as simple as uploading the following files on your HuggingFace Space:</p><ul><li><code>app.py</code> - This file contains the code of the app.</li><li><code>requirements.txt</code> - This file lists the dependencies for the app.</li></ul><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Upload the files for the Gradio app on HuggingFace Spaces | Image by Author</figcaption></figure><p>The deployed app will expect you to pass in the input text or prompt, which itâ€™ll then use to generate an appropriate response.</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Gradio app with the Vicuna model | Image by Author</figcaption></figure><h3 id="accessing-the-llm-as-an-api-endpoint">Accessing the LLM as an API Endpoint:</h3><p>The deployed Gradio app is already running a Prediction (Inference) API endpoint in the background.<br />The endpoint can be easily accessed through the <a href="https://gradio.app/getting-started-with-the-python-client/?ref=localhost">Gradio Python Client</a>.</p><p>At the bottom of the deployed app, you will see a link called "Use via API". Click this link to view the instructions on how to call your app with the API.</p><figure class="kg-card kg-image-card kg-card-hascaption"><figcaption>Using the API Endpoints to access the LLM | Image by Author</figcaption></figure><p>To use the API, you will need to install the Gradio Client Python library. You can do this by running the following command in your terminal:</p><pre><code class="language-bash">pip install gradio_client
</code></pre><p>Once you have installed the library, you can use any of the deployed apps for generating text similar to the OpenAI completion endpoints in the following manner:</p><pre><code class="language-python">from gradio_client import Client

# Pass the link to your HuggingFace Space here
client = Client("https://awinml-falcon-7b-instruct-api.hf.space/")

# Pass the Input Prompt to the model
result = client.predict(
Â  Â  "What is the capital of USA?",
Â  Â  api_name="/predict"
)
print(result)
</code></pre><p>This code will first create a Client object and pass the link to your HuggingFace Space to it. Then, it will pass the input prompt to the model and call the <code>predict()</code> method. The <code>predict()</code> method will return the generated text, which you can then print to the console.</p><h2 id="conclusion">Conclusion</h2><p>Now, you can deploy any large language model (LLM) as an API endpoint with just a few lines of code, thanks to Gradio and HuggingFace Spaces. These tools make it simple to build your own free text generation endpoints. By deploying your own LLM on an API endpoint, you can save money by avoiding costly paid APIs while still benefiting from the remarkable capabilities of these powerful language models.</p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
            <amp-img class="site-icon" src="https://awinml.github.io/content/images/2023/06/android-chrome-192x192.png" width="50" height="50" layout="fixed" alt="Ashwin Mathur"></amp-img>
        <h3>Ashwin Mathur</h3>
            <p>AI Research, NLP, Open-Source</p>
        <p><a href="https://awinml.github.io">Read more posts â†’</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
            <amp-analytics type="gtag" data-credentials="include">
                <script type="application/json">
                    {
                        "vars" : {
                            "gtag_id": "G-TQJMQLKYQ9",
                            "config" : {
                                "G-TQJMQLKYQ9": { "groups": "default" }
                            }
                        }
                    }
                </script>
            </amp-analytics>
        
</body>
</html>
