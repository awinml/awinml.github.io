<!DOCTYPE html>
<html lang="en" class="auto-color">
<head>

    <title>How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css?v=29c90eb8f0" />

    <meta name="description" content="Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.">
    <link rel="icon" href="https://awinml.github.io/content/images/size/w256h256/2023/06/android-chrome-192x192.png" type="image/png">
    <link rel="canonical" href="https://awinml.github.io/llm-ggml-python/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="https://awinml.github.io/llm-ggml-python/amp/">
    
    <meta property="og:site_name" content="Ashwin Mathur">
    <meta property="og:type" content="article">
    <meta property="og:title" content="How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide">
    <meta property="og:description" content="Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.">
    <meta property="og:url" content="https://awinml.github.io/llm-ggml-python/">
    <meta property="og:image" content="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta property="article:published_time" content="2023-06-30T20:38:39.000Z">
    <meta property="article:modified_time" content="2023-07-01T10:24:26.000Z">
    <meta property="article:tag" content="Blog">
    
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide">
    <meta name="twitter:description" content="Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.">
    <meta name="twitter:url" content="https://awinml.github.io/llm-ggml-python/">
    <meta name="twitter:image" content="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Ashwin Mathur">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Blog">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="1333">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Ashwin Mathur",
        "url": "https://awinml.github.io/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://awinml.github.io/content/images/size/w256h256/2023/06/android-chrome-192x192.png",
            "width": 60,
            "height": 60
        }
    },
    "author": {
        "@type": "Person",
        "name": "Ashwin Mathur",
        "url": "https://awinml.github.io/author/ashwin/",
        "sameAs": []
    },
    "headline": "How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide",
    "url": "https://awinml.github.io/llm-ggml-python/",
    "datePublished": "2023-06-30T20:38:39.000Z",
    "dateModified": "2023-07-01T10:24:26.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&ixlib=rb-4.0.3&q=80&w=2000",
        "width": 2000,
        "height": 1333
    },
    "keywords": "Blog",
    "description": "Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.",
    "mainEntityOfPage": "https://awinml.github.io/llm-ggml-python/"
}
    </script>

    <meta name="generator" content="Ghost 5.49">
    <link rel="alternate" type="application/rss+xml" title="Ashwin Mathur" href="https://awinml.github.io/rss/">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="3f8153ddbf039121b27e361e3a" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="https://awinml.github.io/" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/ghost/announcement-bar@~1.1/umd/announcement-bar.min.js" data-announcement-bar="https://awinml.github.io/" data-api-url="https://awinml.github.io/members/api/announcement/" crossorigin="anonymous"></script>
    <link href="https://awinml.github.io/webmentions/receive/" rel="webmention">
    <script defer src="/public/cards.min.js?v=29c90eb8f0"></script>
    <link rel="stylesheet" type="text/css" href="/public/cards.min.css?v=29c90eb8f0">
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8005664633543622"
     crossorigin="anonymous"></script>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FC0MGC2F3F"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FC0MGC2F3F');
</script>


<!-- Social Media Icons -->

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/brands.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
<style>
    .gh-head-menu .nav-email a,
    .gh-head-menu .nav-github a,
    .gh-head-menu .nav-medium a,
    .gh-head-menu .nav-linkedin a {
        font-size: 0 !important;
    }

    .gh-head-menu .nav-email a::before {
        font-family: "Font Awesome 6 Free";
        display: inline-block;
        font-size: 20px;
        font-style: normal;
        font-weight: normal;
        font-variant: normal;
        text-rendering: auto;
        -webkit-font-smoothing: antialiased;
    }
 	
    .gh-head-menu .nav-medium a::before,
    .gh-head-menu .nav-github a::before,
    .gh-head-menu .nav-linkedin a::before {
        font-family: "Font Awesome 6 Brands";
        display: inline-block;
        font-size: 20px;
        font-style: normal;
        font-weight: normal;
        font-variant: normal;
        text-rendering: auto;
        -webkit-font-smoothing: antialiased;
    }

    .gh-head-menu .nav-email a::before {content: "\f0e0"}
    .gh-head-menu .nav-github a::before {content: "\f09b"}
    .gh-head-menu .nav-linkedin a::before {content: "\f08c"}
    .gh-head-menu .nav-medium a::before {content: "\f23a"}
    
</style>

<style>:root {--ghost-accent-color: #023e8a;}</style>

</head>
<body class="post-template tag-blog is-head-left-logo has-cover">
<div class="viewport">

    <header id="gh-head" class="gh-head outer">
        <div class="gh-head-inner inner">
            <div class="gh-head-brand">
                <a class="gh-head-logo no-image" href="https://awinml.github.io">
                        Ashwin Mathur
                </a>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
                <button class="gh-burger"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-home"><a href="https://awinml.github.io/">Home</a></li>
    <li class="nav-about"><a href="https://awinml.github.io/about/">About</a></li>
    <li class="nav-blog"><a href="https://awinml.github.io/tag/blog/">Blog</a></li>
    <li class="nav-projects"><a href="https://awinml.github.io/tag/projects/">Projects</a></li>
    <li class="nav-email"><a href="mailto:ashwinmathur.business@gmail.com">Email</a></li>
    <li class="nav-github"><a href="https://github.com/awinml">GitHub</a></li>
    <li class="nav-linkedin"><a href="https://www.linkedin.com/in/ashwin-mathur-ds/">LinkedIn</a></li>
    <li class="nav-medium"><a href="https://awinml.medium.com/">Medium</a></li>
    <li class="nav-contact"><a href="https://awinml.github.io/contact/">Contact</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
            </div>
        </div>
    </header>

    <div class="site-content">
        



<main id="site-main" class="site-main">
<article class="article post tag-blog featured image-small">

    <header class="article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-primary-tag">
                    <a href="/tag/blog/">Blog</a>
                </span>
                <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
        </div>

        <h1 class="article-title">How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide</h1>

            <p class="article-excerpt">Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. In this blog post, we will see how to use the llama.cpp library in Python to run LLMs on CPUs with high performance.</p>

        <div class="article-byline">
        <section class="article-byline-content">

            <ul class="author-list">
                <li class="author-list-item">
                    <a href="/author/ashwin/" class="author-avatar author-profile-image"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="M3.513 18.998C4.749 15.504 8.082 13 12 13s7.251 2.504 8.487 5.998C18.47 21.442 15.417 23 12 23s-6.47-1.558-8.487-4.002zM12 12c2.21 0 4-2.79 4-5s-1.79-4-4-4-4 1.79-4 4 1.79 5 4 5z" fill="#FFF"/></g></svg>
</a>
                </li>
            </ul>

            <div class="article-byline-meta">
                <h4 class="author-name"><a href="/author/ashwin/">Ashwin Mathur</a></h4>
                <div class="byline-meta-content">
                    <time class="byline-meta-date" datetime="2023-07-01">Jul 1, 2023</time>
                        <span class="byline-reading-time"><span class="bull">&bull;</span> 7 min read</span>
                </div>
            </div>

        </section>
        </div>

            <figure class="article-image">
                <img
                    srcset="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;300 300w,
                            https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;600 600w,
                            https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1000 1000w,
                            https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000 2000w"
                    sizes="(min-width: 1400px) 1400px, 92vw"
                    src="https://images.unsplash.com/photo-1526308422422-6a57b9567eff?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDE4fHxsbGFtYSUyMGF0JTIwYSUyMGRpc3RhbmNlfGVufDB8fHx8MTY4ODE1NzE0OXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000"
                    alt="How to Run LLMs on Your CPU with Llama.cpp: A Step-by-Step Guide"
                />
                    <figcaption>Photo by <a href="https://unsplash.com/@willianjusten?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Willian Justen de Vasconcellos</a> / <a href="https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit">Unsplash</a></figcaption>
            </figure>

    </header>

    <section class="gh-content gh-canvas">
        <!--kg-card-begin: markdown--><p>Large language models (LLMs) are becoming increasingly popular, but they can be computationally expensive to run. There have been several advancements like the support for 4-bit and 8-bit loading of models on HuggingFace. But they require a GPU to work. This has limited their use to people with access to specialized hardware, such as GPUs. Even though it is possible to run these LLMs on CPUs, the performance is limited and hence restricts the usage of these models.</p>
<p>Recent work by <a href="https://github.com/ggerganov?ref=localhost">Georgi Gerganov</a> has made it possible to run LLMs on CPUs with high performance. This is thanks to his implementation of the <a href="https://github.com/ggerganov/llama.cpp?ref=localhost">llama.cpp</a> library, which provides high-speed inference for a variety of LLMs.</p>
<p>The original llama.cpp library focuses on running the models locally in a shell. This does not offer a lot of flexibility to the user and makes it hard for the user to leverage the vast range of python libraries to build applications. Recently LLM frameworks like <a href="https://python.langchain.com/docs/get_started/introduction.html?ref=localhost">LangChain</a> have added support for llama.cpp using the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">llama-cpp-python</a> package.</p>
<p>In this blog post, we will see how to use the llama.cpp library in Python using the <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost">llama-cpp-python</a> package. This package provides Python bindings for llama.cpp, which makes it easy to use the library in Python.</p>
<p>We will also see how to use the llama-cpp-python library to run the <a href="https://lmsys.org/blog/2023-03-30-vicuna/?ref=localhost">Vicuna  LLM</a>, which is an open-source model based on the <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/?ref=localhost">LLaMA</a> architecture that behaves like ChatGPT.</p>
<h2 id="set-up-llama-cpp-python">Set up llama-cpp-python</h2>
<p>Setting up the python bindings is as simple as running the following command:</p>
<pre><code class="language-bash">pip install llama-cpp-python
</code></pre>
<p>For more detailed installation instructions, please see the llama-cpp-python documentation: <a href="https://github.com/abetlen/llama-cpp-python?ref=localhost#installation-from-pypi-recommended">https://github.com/abetlen/llama-cpp-python#installation-from-pypi-recommended</a>.</p>
<h2 id="using-a-llm-with-llama-cpp-python">Using a LLM with llama-cpp-python</h2>
<p>Once you have installed the llama-cpp-python package, you can start using it to run LLMs.</p>
<p>You can use any language model with llama.cpp provided that it has been converted to the GGML format. There are already GGML versions available for most popular LLMs and the required GGML can be easily found on HuggingFace.</p>
<p>An important thing to note is that the original LLMs have been quantized when converting them to GGML format. This helps reduce the memory requirement for running these large models, without a significant loss in performance. For example, this helps us load a 7 billion parameter model of size 13GB in less than 4GB of RAM.</p>
<p>In this article we use the GGML version of Vicuna-7B which is available on the Hugging Face Hub.</p>
<p>The model can be downloaded from here: <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized</a>.</p>
<h3 id="downloading-the-ggml-file-and-loading-the-llm">Downloading the GGML file and Loading the LLM</h3>
<p>The following code can be used to download the model. The code downloads the required GGML file, in this case the <a href="https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized?ref=localhost">Vicuna-7b-Q4.1 GGML</a>, from the Hugging Face Hub. The code also checks if the file is already present before attempting to download it.</p>
<pre><code class="language-python">import os
import urllib.request


def download_file(file_link, filename):
    # Checks if the file already exists before downloading
    if not os.path.isfile(filename):
        urllib.request.urlretrieve(file_link, filename)
        print(&quot;File downloaded successfully.&quot;)
    else:
        print(&quot;File already exists.&quot;)

# Dowloading GGML model from HuggingFace
ggml_model_path = &quot;https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin&quot;
filename = &quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;

download_file(ggml_model_path, filename)
</code></pre>
<p>The next step is to load the model that you want to use. This can be done using the following code:</p>
<pre><code class="language-python">from llama_cpp import Llama

llm = Llama(model_path=&quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;, n_ctx=512, n_batch=126)
</code></pre>
<p>There are two important parameters that should be set when loading the model.</p>
<ul>
<li><code>n_ctx</code>: This is used to set the maximum context size of the model. The default value is 512 tokens.</li>
</ul>
<p>The context size is the sum of the number of tokens in the input prompt and the max number of tokens that can be generated by the model. A model with smaller context size generates text much quicker than a model with a larger context size. If the use case does not demand very long generations or prompts, it is better to reduce the context length for better performance.</p>
<p>The number of tokens in the prompt and generated text can be checked using the free <a href="https://platform.openai.com/tokenizer?ref=localhost">Tokenizer tool by OpenAI</a>.</p>
<ul>
<li><code>n_batch</code>: This is used to set the maximum number of prompt tokens to batch together when generating the text. The default value is 512 tokens.</li>
</ul>
<p>The <code>n_batch</code> parameter should be set carefully. Lowering the <code>n_batch</code> helps speed up text generation over multithreaded CPUs. Reducing it too much may cause the text generation to deteriorate significantly.</p>
<p>The complete list of parameters can be viewed here: <a href="https://llama-cpp-python.readthedocs.io/en/latest/api-reference/?ref=localhost#llama_cpp.Llama">https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.Llama</a></p>
<h3 id="generating-text-using-the-llm">Generating Text using the LLM</h3>
<p>The following code writes a simple wrapper function to generate text using the LLM.</p>
<pre><code class="language-python">def generate_text(
    prompt=&quot;Who is the CEO of Apple?&quot;,
    max_tokens=256,
    temperature=0.1,
    top_p=0.5,
    echo=False,
    stop=[&quot;#&quot;],
):
    output = llm(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        echo=echo,
        stop=stop,
    )
    output_text = output[&quot;choices&quot;][0][&quot;text&quot;].strip()
    return output_text
</code></pre>
<p>The <code>llm</code> object has several important parameters that are used while generating text:</p>
<ul>
<li>
<p><code>prompt</code>: The input prompt to the model. This text is tokenized and passed to the model.</p>
</li>
<li>
<p><code>max_tokens</code>: The parameter is used to set the maximum number of tokens the model can generate. This parameter controls the length of text generation. Default value is 128 tokens.</p>
</li>
<li>
<p><code>temperature</code>: The token sampling temperature to use, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. Default value is 1.</p>
</li>
<li>
<p><code>top_p</code>: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</p>
</li>
<li>
<p><code>echo</code>: Boolean parameter to control whether the model returns (echoes) the model prompt at the beginning of the generated text.</p>
</li>
<li>
<p><code>stop</code>: A list of strings that is used to stop text generation. If the model encounters any of the strings, the text generation will be stopped at that token. Used to control model hallucination and prevent the model from generating unnecessary text.</p>
</li>
</ul>
<p>The <code>llm</code> object returns a dictionary object of the form:</p>
<pre><code class="language-python">{
  &quot;id&quot;: &quot;xxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx&quot;,  # text generation id 
  &quot;object&quot;: &quot;text_completion&quot;,              # object name
  &quot;created&quot;: 1679561337,                    # time stamp
  &quot;model&quot;: &quot;./models/7B/ggml-model.bin&quot;,    # model path
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.&quot;, # generated text
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: None,
      &quot;finish_reason&quot;: &quot;stop&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 14,       # Number of tokens present in the prompt
    &quot;completion_tokens&quot;: 28,   # Number of tokens present in the generated text
    &quot;total_tokens&quot;: 42
  }
}
</code></pre>
<p>The generated text can be easily extracted from the dictionary object using <code>output[&quot;choices&quot;][0][&quot;text&quot;]</code>.</p>
<h3 id="example-text-generation-using-vicuna-7b">Example text generation using Vicuna-7B</h3>
<pre><code class="language-python">import os
import urllib.request
from llama_cpp import Llama


def download_file(file_link, filename):
    # Checks if the file already exists before downloading
    if not os.path.isfile(filename):
        urllib.request.urlretrieve(file_link, filename)
        print(&quot;File downloaded successfully.&quot;)
    else:
        print(&quot;File already exists.&quot;)


# Dowloading GGML model from HuggingFace
ggml_model_path = &quot;https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/resolve/main/ggml-vicuna-7b-1.1-q4_1.bin&quot;
filename = &quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;

download_file(ggml_model_path, filename)


llm = Llama(model_path=&quot;ggml-vicuna-7b-1.1-q4_1.bin&quot;, n_ctx=512, n_batch=126)


def generate_text(
    prompt=&quot;Who is the CEO of Apple?&quot;,
    max_tokens=256,
    temperature=0.1,
    top_p=0.5,
    echo=False,
    stop=[&quot;#&quot;],
):
    output = llm(
        prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        echo=echo,
        stop=stop,
    )
    output_text = output[&quot;choices&quot;][0][&quot;text&quot;].strip()
    return output_text


generate_text(
    &quot;Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.&quot;,
    max_tokens=356,
)
</code></pre>
<p>Generated text:</p>
<pre><code>Hawaii is a state located in the United States of America that is known for its beautiful beaches, lush landscapes, and rich culture. It is made up of six islands: Oahu, Maui, Kauai, Lanai, Molokai, and Hawaii (also known as the Big Island). Each island has its own unique attractions and experiences to offer visitors.
One of the most interesting cultural experiences in Hawaii is visiting a traditional Hawaiian village or ahupuaa. An ahupuaa is a system of land use that was used by ancient Hawaiians to manage their resources sustainably. It consists of a coastal area, a freshwater stream, and the surrounding uplands and forests. Visitors can learn about this traditional way of life at the Polynesian Cultural Center in Oahu or by visiting a traditional Hawaiian village on one of the other islands.
Another must-see attraction in Hawaii is the Pearl Harbor Memorial. This historic site commemorates the attack on Pearl Harbor on December 7, 1941, which led to the United States' entry into World War II. Visitors can see the USS Arizona Memorial, a memorial that sits above the sunken battleship USS Arizona and provides an overview of the attack. They can also visit other museums and exhibits on the site to learn more about this important event in American history.
Hawaii is also known for its beautiful beaches and crystal clear waters, which are perfect for swimming, snorkeling, and sunbathing.
</code></pre>
<p>The notebook with the example can be viewed <a href="https://nbviewer.org/github/awinml/llama-cpp-python-bindings/blob/main/vicuna-7b-example-ggml.ipynb?ref=localhost">here</a>.</p>
<p>The complete code for running the examples can be found on <a href="https://github.com/awinml/llama-cpp-python-bindings?ref=localhost">GitHub</a>.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this blog post, we explored how to use the llama.cpp library in Python with the llama-cpp-python package. These tools enable high-performance CPU-based execution of LLMs.</p>
<p>llama.cpp is updated almost every day. The speed of inference is getting better, and the community regularly adds support for new models. You can also convert your own Pytorch language models into the ggml format. llama.cpp has a “convert.py” that will do that for you.</p>
<p>The llama.cpp library and llama-cpp-python package provide robust solutions for running LLMs efficiently on CPUs. If you're interested in incorporating LLMs into your applications, I recommend exploring these resources.</p>
<!--kg-card-end: markdown-->
    </section>


</article>
</main>




            <aside class="read-more-wrap outer">
                <div class="read-more inner">
                        
<article class="post-card post featured">

    <a class="post-card-image-link" href="/llm-text-gen-api/">

        <img class="post-card-image"
            srcset="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;300 300w,
                    https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;600 600w,
                    https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1000 1000w,
                    https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="https://images.unsplash.com/photo-1676573409967-986dcf64d35a?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDUyfHxncHR8ZW58MHx8fHwxNjg4MjA2NDA4fDA&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;600"
            alt="No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/llm-text-gen-api/">
            <header class="post-card-header">
                <div class="post-card-tags">
                        <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
                </div>
                <h2 class="post-card-title">
                    No More Paid Endpoints: How to Create Your Own Free Text Generation Endpoints with Ease
                </h2>
            </header>
                <div class="post-card-excerpt">One of the biggest challenges of using LLMs is the cost of accessing them. Many LLMs, such as OpenAI's GPT-3, are only available through paid APIs. In this article, we see how to deploy any open-source LLM as a free API endpoint using HuggingFace and Gradio.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2023-07-01">Jul 1, 2023</time>
                <span class="post-card-meta-length">7 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post">

    <a class="post-card-image-link" href="/ml-model-deployment/">

        <img class="post-card-image"
            srcset="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;300 300w,
                    https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;600 600w,
                    https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;1000 1000w,
                    https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;2000 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="https://images.unsplash.com/photo-1554306274-f23873d9a26c?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;M3wxMTc3M3wwfDF8c2VhcmNofDgyfHxjb2RlfGVufDB8fHx8MTY4Njk5MTM3MXww&amp;ixlib&#x3D;rb-4.0.3&amp;q&#x3D;80&amp;w&#x3D;600"
            alt="Machine Learning Model Deployment: From Jupyter Notebook to the Cloud"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/ml-model-deployment/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Machine Learning Model Deployment: From Jupyter Notebook to the Cloud
                </h2>
            </header>
                <div class="post-card-excerpt">Building a machine learning model is only half the battle. Deploying the model into a production environment where it can be used to make predictions is equally important. In this article, we will explore the steps involved in deploying a machine learning model from a Jupyter Notebook to the cloud.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2023-06-17">Jun 17, 2023</time>
                <span class="post-card-meta-length">4 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post">

    <a class="post-card-image-link" href="/financial-dashboard-for-market-intelligence/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2023/06/fin_dash_cover.png 300w,
                    /content/images/size/w600/2023/06/fin_dash_cover.png 600w,
                    /content/images/size/w1000/2023/06/fin_dash_cover.png 1000w,
                    /content/images/size/w2000/2023/06/fin_dash_cover.png 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/content/images/size/w600/2023/06/fin_dash_cover.png"
            alt="Financial Dashboard for Market Intelligence"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/financial-dashboard-for-market-intelligence/">
            <header class="post-card-header">
                <div class="post-card-tags">
                </div>
                <h2 class="post-card-title">
                    Financial Dashboard for Market Intelligence
                </h2>
            </header>
                <div class="post-card-excerpt">Built a end-to-end financial dashboard that collects and consolidates all of a business's critical observations in one place using the information obtained from the annual 10-K SEC Filings of 12 companies.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2023-06-02">Jun 2, 2023</time>
                <span class="post-card-meta-length">3 min read</span>
        </footer>

    </div>

</article>
                </div>
            </aside>



    </div>

    <footer class="site-footer outer">
        <div class="inner">
            <section class="copyright"><a href="https://awinml.github.io">Ashwin Mathur</a> &copy; 2023</section>
            <nav class="site-footer-nav">
                <ul class="nav">
    <li class="nav-linktree"><a href="https://awinml.github.io/linktree/">LinkTree</a></li>
    <li class="nav-github"><a href="https://github.com/awinml">GitHub</a></li>
    <li class="nav-linkedin"><a href="https://www.linkedin.com/in/ashwin-mathur-ds/">LinkedIn</a></li>
    <li class="nav-email"><a href="mailto:ashwinmathur.business@gmail.com">Email</a></li>
    <li class="nav-medium"><a href="https://awinml.medium.com/">Medium</a></li>
    <li class="nav-contact"><a href="https://awinml.github.io/contact/">Contact</a></li>
</ul>

            </nav>
            <div class="gh-powered-by"><a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a></div>
        </div>
    </footer>

</div>


<script
    src="https://code.jquery.com/jquery-3.5.1.min.js"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous">
</script>
<script src="/assets/built/casper.js?v=29c90eb8f0"></script>
<script>
$(document).ready(function () {
    // Mobile Menu Trigger
    $('.gh-burger').click(function () {
        $('body').toggleClass('gh-head-open');
    });
    // FitVids - Makes video embeds responsive
    $(".gh-content").fitVids();
});
</script>



</body>
</html>
